I"7<h2 id="paper-info">Paper Info</h2>

<p><a href="https://arxiv.org/abs/1802.05365">Archive Link</a></p>

<p><a href="https://arxiv.org/pdf/1802.05365.pdf">Paper Link</a></p>

<p>Submit Date: Feb 15, 2018</p>

<hr />

<h1 id="introduction">Introduction</h1>

<p>word2vecì´ë‚˜ gloveì™€ ê°™ì€ ê¸°ì¡´ì˜ word embedding ë°©ì‹ì€ ë‹¤ì˜ì–´ì˜ ëª¨ë“  ì˜ë¯¸ë¥¼ ë‹´ì•„ë‚´ê¸° ê³¤ë€í•˜ë‹¤ëŠ” ì‹¬ê°í•œ í•œê³„ì ì„ ê°–ê³  ìˆë‹¤. ELMo(Embeddings from Language Models)ëŠ” ì´ëŸ¬í•œ í•œê³„ì ì„ ê·¹ë³µí•˜ê¸° ìœ„í•´ embeddingì— sentenceì˜ ì „ì²´ contextë¥¼ ë‹´ë„ë¡ í–ˆë‹¤. pre-trainëœ LSTM layerì— sentence ì „ì²´ë¥¼ ë„£ì–´ ê° wordì˜ embeddingì„ êµ¬í•´ë‚´ëŠ” modelì´ë‹¤. ì´ë¡œ ì¸í•´ ê¸°ì¡´ì˜ embeddingì— ë¹„í•´ ë³µì¡í•œ syntax, semanticí•œ íŠ¹ì§•ë“¤ì„ ë‹´ì•„ë‚¼ ìˆ˜ ìˆì—ˆê³ , ë‹¤ì˜ì–´ ë¬¸ì œë„ í•´ê²°í–ˆë‹¤. LSTMì€ forward, backward 2ê°€ì§€ ë°©í–¥ì„ ì‚¬ìš©í•˜ëŠ”ë° ê° LSTMì€ ë‹¤ìŒ wordë¥¼ ì˜ˆì¸¡í•˜ëŠ” modelì´ë‹¤. ì´ëŸ¬í•œ LSTM layerë¥¼ ë‹¤ì¸µìœ¼ë¡œ êµ¬ì„±í•´ context-dependentí•œ word embeddingì„ ìƒì„±í•´ë‚´ê²Œ ëœë‹¤. ELMoë¥¼ ì‚¬ìš©í•œ word embeddingì€ ìˆ˜ë§ì€ NLP taskì—ì„œ ì„±ëŠ¥ í–¥ìƒì„ ì´ëŒì–´ë‚¼ ìˆ˜ ìˆë‹¤.</p>

<h1 id="elmo-embeddings-from-language-models">ELMo: Embeddings from Language Models</h1>

<p>ELMoì™€ ì—¬íƒ€ word embedding modelê³¼ì˜ ê°€ì¥ í° ì°¨ì´ì ì€ ê° ë‹¨ì–´ë§ˆë‹¤ ê³ ì •ëœ word embeddingì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì•„ë‹Œ pre-trained model ìì²´ë¥¼ í•˜ë‚˜ì˜ function ê°œë…ìœ¼ë¡œ ì‚¬ìš©í•œë‹¤ëŠ” ê²ƒì´ë‹¤. ì´ì „ì˜ word2vec ë“±ì€ modelì„ trainì‹œí‚¨ ë’¤ ê° wordë§ˆë‹¤ì˜ embedding vectorë§Œì„ ì¶”ì¶œí•´ ì‚¬ìš©í–ˆì§€ë§Œ, ELMoëŠ” wordë§ˆë‹¤ embedding vectorê°€ íŠ¹ì •ë˜ì§€ ì•Šê¸°ì— ì´ì™€ ê°™ì€ ë°©ì‹ì´ ë¶ˆê°€ëŠ¥í•˜ê³ , ELMo modelì„ NLP modelì˜ ì•ì— ì—°ê²°í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì‚¬ìš©í•˜ê²Œ ëœë‹¤.</p>

<h2 id="bidirectional-language-models">Bidirectional language models</h2>

<p>input sequenceê°€ \(N\)ê°œì˜ tokenë“¤ (\(t_1, t_2, ..., t_N\))ì´ë¼ê³  ê°€ì •í•˜ì. \(t_k\)ëŠ” ëª¨ë‘ ê° wordì— ëŒ€í•œ context-independent tokenì´ë‹¤.</p>

<p>forward LSTMì€ \(t_1, ... , t_{k-1}\)ì´ ì£¼ì–´ì¡Œì„ ë•Œ \(t_k\)ë¥¼ ì˜ˆì¸¡í•˜ëŠ” modelì´ë‹¤.</p>

<p>\(j\)ë²ˆì§¸ LSTM layerì—ì„œ \(k\)ë²ˆì§¸ tokenì— ëŒ€í•œ forward LSTM outputì€ \(\overrightarrow{h}_{k,j}\)ì´ë‹¤. ë§ˆì§€ë§‰ LSTM layerì˜ outputì¸ \(\overrightarrow{h}_{k,L}\)ì„ softmaxì— ë„£ì–´ ìµœì¢…ì ìœ¼ë¡œ \(t_{k+1}\)ì„ ì˜ˆì¸¡í•˜ê²Œ ëœë‹¤.</p>

\[p(t_1, t_2, ... , t_N) = \prod^N_{k=1}{p(t_k \vert t_1, t_2, ..., t_{k-1})}\]

<p>backward LSTMì€ \(t_{k+1}, t_{k+2}, ... ,t_N\)ì´ ì£¼ì–´ì¡Œì„ ë•Œ \(t_k\)ë¥¼ ì˜ˆì¸¡í•˜ëŠ” modelì´ë‹¤.</p>

<p>\(j\)ë²ˆì§¸ LSTM layerì—ì„œ \(k\)ë²ˆì§¸ tokenì— ëŒ€í•œ backward LSTM outputì€ \(\overleftarrow{h}_{k,j}\)ì´ë‹¤. ë§ˆì§€ë§‰ LSTM layerì˜ outputì¸ \(\overleftarrow{h}_{k,L}\)ì„ softmaxì— ë„£ì–´ ìµœì¢…ì ìœ¼ë¡œ \(t_{k-1}\)ì„ ì˜ˆì¸¡í•˜ê²Œ ëœë‹¤.</p>

<p>\(\)p(t_1, t_2, â€¦ , t_N) = \prod^N_{k=1}{p(t_k \vert t_{k+1}, t_{k+2}, â€¦, t_{N})}\(\)</p>

<p>biLMì€ ìœ„ì˜ ë‘ LSTMì„ ê²°í•©í•œ ê²ƒì´ë‹¤. ë‘ ë°©í–¥ì— ëŒ€í•œ log likelihoodë¥¼ ìµœëŒ€í™”í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•œë‹¤.</p>

\[\sum^N_{k=1} {(\log p(t_k \vert t_1, ... , t_{k-1} ; \Theta_x, \overrightarrow{\Theta}_{\text{LSTM}}, \Theta_s) + \log p(t_k \vert t_{k+1}, ... , t_{N} ; \Theta_x, \overleftarrow{\Theta}_{\text{LSTM}}, \Theta_s))}\]

<p>\(\Theta_x\)ëŠ” token representation(\(t_1, ... , t_N\))ì— ëŒ€í•œ parameterì´ê³ , \(\Theta_s\)ëŠ” softmax layerì— ëŒ€í•œ parameterì´ë‹¤. ì´ ë‘ parameterëŠ” ì „ì²´ directionì— ê´€ê³„ ì—†ì´ ê°™ì€ ê°’ì„ ê³µìœ í•˜ì§€ë§Œ, LSTMì˜ parameterë“¤ì€ ë‘ LSTM modelì´ ì„œë¡œ ë‹¤ë¥¸ ê°’ì„ ê°–ëŠ”ë‹¤.</p>

<h2 id="elmo">ELMo</h2>

<p>ELMoì—ì„œëŠ” ìƒˆë¡œìš´ representationì„ ì‚¬ìš©í•˜ëŠ”ë°, ì´ë¥¼ ì–»ê¸° ìœ„í•´ì„œëŠ” LSTM layerì˜ ê°œìˆ˜ë¥¼ \(L\)ì´ë¼ê³  í–ˆì„ ë•Œ ì´ \(2L+1\)ê°œì˜ representationì„ concatenateí•´ì•¼ í•œë‹¤. input representation layer 1ê°œì™€ forward, backward LSTM ê°ê° \(L\)ê°œì´ë‹¤.</p>

<p>\(\)R_k = {x_k, \overrightarrow{h}<em>{k,j}, \overleftarrow{h}</em>{k,j} \vert j=1, â€¦ , L}\(\)</p>

<p>input representation layerë¥¼ \(j=0\)ìœ¼ë¡œ, \(\overrightarrow{h}_{k,j}\)ì™€ \(\overleftarrow{h}_{k,j}\)ì˜ concatenationì„ \(h_{k,j}\)ë¡œ í‘œí˜„í•œë‹¤ë©´ ë‹¤ìŒê³¼ ê°™ì€ ì¼ë°˜í™”ëœ ìˆ˜ì‹ìœ¼ë¡œ ELMO representationì„ í‘œí˜„í•  ìˆ˜ ìˆë‹¤.</p>

\[R_k = \{h_{k,j} \vert j=0, ..., L\}\]

<p>ê²°êµ­ \(R_k\)ëŠ” \(k\)ë²ˆì§¸ tokenì— ëŒ€í•œ ëª¨ë“  representationì´ ì—°ê²°ë˜ì–´ ìˆëŠ” ê²ƒì¸ë°, ì´ë¥¼ ì‚¬ìš©í•´ ìµœì¢… ELMo embedding vectorë¥¼ ë§Œë“¤ì–´ë‚´ê²Œ ëœë‹¤.</p>

\[\text{ELMo}^{task}_k=E(R_k;\Theta^{task}) = \gamma^{task} \sum^L_{j=0} {s_j^{task}h_{k,j}}\]

<p>ê° LSTM layerì˜ outputì¸ \(h_{k,j}\)ë¥¼ ëª¨ë‘ ë”í•˜ëŠ”ë° ì´ ë•Œ ê°ê°ì— softmax-normalized weights \(s_j\)ë¥¼ ê³±í•œ ë’¤ ë”í•˜ê²Œ ëœë‹¤. ë‹¹ì—°í•˜ê²Œë„ \(\sum^L_{j=0}s_j^{task}=1\)ì´ë‹¤. ë§ˆì§€ë§‰ì— ìµœì¢…ì ìœ¼ë¡œ \(\gamma\)ë¥¼ ê³±í•˜ê²Œ ë˜ëŠ”ë°, scale parameterì´ë‹¤. \(s_j\)ì™€ \(\gamma\)ëŠ” ëª¨ë‘ learnable parameterì´ë©´ì„œ optimizationì—ì„œ ë§¤ìš° ì¤‘ìš”í•œ ì—­í• ì„ ë‹´ë‹¹í•œë‹¤.</p>

<h2 id="using-bilms-for-supervised-nlp-tasks">Using biLMs for supervised NLP tasks</h2>

<p>supervised downstream taskì— ELMoë¥¼ ì ìš©í•˜ëŠ” êµ¬ì²´ì ì¸ ë°©ë²•ì€ ê°„ë‹¨í•˜ë‹¤. ëŒ€ë¶€ë¶„ì˜ supervised NLP modelë“¤ì˜ inputì€ ëª¨ë‘ context-independent tokenë“¤ì˜ sequenceì´ë‹¤. ì´ëŸ¬í•œ ê³µí†µì  ë•ë¶„ì— ë™ì¼í•œ ë°©ë²•ìœ¼ë¡œ ëŒ€ë¶€ë¶„ì˜ taskì— ELMoë¥¼ ì ìš©í•  ìˆ˜ ìˆë‹¤. ìš°ì„  NLP modelì˜ input layerì™€ ë‹¤ìŒ layer ì‚¬ì´ì— ELMoë¥¼ ì‚½ì…í•œë‹¤. ì´í›„ ELMo ì´í›„ layerì—ì„œëŠ” inputì„ \([x_k;\text{ELMo}_k^{task}]\)ë¡œ ì‚¬ìš©í•œë‹¤. ì¦‰, input tokenê³¼ ELMo embedding vectorì˜ concatenationì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ë‹¤. NLP modelì„ trainí•  ë•Œì—ëŠ” ELMoì˜ weightë“¤ì€ ëª¨ë‘ freezeì‹œí‚¨ë‹¤. ì¦‰, ELMo modelì€ NLP modelì´ trainë  ë•Œ í•¨ê»˜ trainë˜ì§€ ì•ŠëŠ”ë‹¤.</p>

<p>SNLI, SQuADì™€ ê°™ì€ íŠ¹ì • taskì—ì„œëŠ” RNNì˜ output \(h_k\)ë¥¼ \([h_k;\text{ELMo}_k^{task}]\)ë¡œ êµì²´í–ˆì„ ë•Œ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì´ê¸°ë„ í–ˆë‹¤. ë˜í•œ ì¼ë°˜ì ìœ¼ë¡œ dropoutê³¼ \(\lambda \Vert w \Vert^2_2\)ë¥¼ lossì— ë”í•˜ëŠ” regularizationì„ ì‚¬ìš©í–ˆì„ ë•Œ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì˜€ë‹¤.</p>

<h2 id="pre-trained-bidirectional-language-model-architecture">Pre-trained bidirectional language model architecture</h2>

<p>ELMoëŠ” ê¸°ì¡´ì˜ pre-trained biLMì™€ í° êµ¬ì¡°ëŠ” ë¹„ìŠ·í•˜ì§€ë§Œ ëª‡ê°€ì§€ ì°¨ì´ì ì´ ì¡´ì¬í•˜ëŠ”ë°, ê°€ì¥ í° ì°¨ì´ì ì€ LSTM layer ì‚¬ì´ì— residual connectionì„ ì‚¬ìš©í–ˆë‹¤ëŠ” ì ì´ë‹¤. ì´ë¥¼ í†µí•´ inputì˜ featureë¥¼ ë” ì˜ ì „ë‹¬í•˜ê³  gradient vanishingì„ í•´ê²°í•  ìˆ˜ ìˆë‹¤. \(L=2\)ê°œì˜ biLSTM layerë¥¼ ì‚¬ìš©í–ˆê³ , 4096ê°œì˜ unitê³¼ 512ì°¨ì›ì˜ projectionì„ ì‚¬ìš©í–ˆë‹¤. biLSTMì˜ inputìœ¼ë¡œëŠ” 2048 character n-gramì„ CNNì— ë„£ëŠ” char-CNN embeddingì„ ì‚¬ìš©í–ˆë‹¤.</p>

<h1 id="evaluation">Evaluation</h1>

<p><img src="/assets/images/2020-01-02-Deep-contextualized-word-representations/01-01-2021-23.00.50.jpg" alt="Deep%20contextualized%20word%20representations%204e91492dc98641ada8f1fdfac764a546/01-01-2021-23.00.50.jpg" /></p>

<p>ELMoë¥¼ ë‹¨ìˆœí•˜ê²Œ ì¶”ê°€í•˜ëŠ” ê²ƒë§Œìœ¼ë¡œë„ baseline modelì— ë¹„í•´ ì„±ëŠ¥ì´ í–¥ìƒëê³ , ì´ë¥¼ í†µí•´ SOTAë¥¼ ë‹¬ì„±í•  ìˆ˜ ìˆì—ˆë‹¤.</p>

<h1 id="analysis">Analysis</h1>

<h2 id="alternate-layer-weighting-schemes">Alternate layer weighting schemes</h2>

<p><img src="/assets/images/2020-01-02-Deep-contextualized-word-representations/01-01-2021-23.01.02.jpg" alt="Deep%20contextualized%20word%20representations%204e91492dc98641ada8f1fdfac764a546/01-01-2021-23.01.02.jpg" /></p>

<p>ELMo representationì„ ì‚¬ìš©í•˜ì§€ ì•Šê³  ë‹¨ìˆœí•˜ê²Œ LSTMì˜ ë§ˆì§€ë§‰ layerì˜ output (\(h_{k,L}\))ì„ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ë„ ìˆë‹¤. ì´ëŸ¬í•œ ë°©ì‹ì€ biLM, CoVe ë“± ê¸°ì¡´ì˜ ë§ì€ ì—°êµ¬ì—ì„œ ì‹œë„ë˜ì—ˆëŠ”ë° ì´ì™€ ELMo representationì„ ì‚¬ìš©í•œ ê²½ìš°ë¥¼ ë¹„êµí•´ë³¸ë‹¤. Table 2ì˜ Last OnlyëŠ” ë§ˆì§€ë§‰ LSTM layerì˜ outputë§Œì„ word embeddingìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” ê²½ìš°ì´ë‹¤.</p>

<p>\(\lambda\)ëŠ” softmax-normalized weights \(s_j\)ì— ëŒ€í•œ regularization parameterì¸ë°, 0~1 ì‚¬ì´ì˜ ê°’ì„ ê°–ëŠ”ë‹¤. \(\lambda\)ê°€ 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ê° layerì—ì„œì˜ outputë“¤ì„ í‰ê· ì— ê°€ê¹ê²Œ ê³„ì‚°í•´ ìµœì¢… vectorë¥¼ ìƒì„±í•´ë‚´ê³  (\(s_j\)ê°€ ëª¨ë‘ ìœ ì‚¬í•œ ê°’), \(\lambda\)ê°€ 0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ê° layerì—ì„œì˜ outputë“¤ì— ë‹¤ì–‘í•œ ê°’ë“¤ì´ ê³±í•´ì„œ ë”í•´ì§„ë‹¤.</p>

<p>Table 2ì—ì„œëŠ” taskì— ê´€ê³„ ì—†ì´ ë™ì¼í•œ ê²½í–¥ì„±ì„ ë³´ì´ëŠ”ë°, baseline modelë³´ë‹¤ CoVeì™€ ê°™ì€ ë§ˆì§€ë§‰ LSTM layerì˜ outputì„ word embeddingìœ¼ë¡œ ì‚¬ìš©í•œ modelì´ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì˜€ë‹¤. ë˜í•œ CoVeë³´ë‹¤ ELMoê°€ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì˜€ëŠ”ë°, ì´ ì¤‘ \(\lambda\)ê°€ ë‚®ì€ ê²½ìš°ê°€ ë” ì¢‹ì€ ê²°ê³¼ë¥¼ ë³´ì˜€ë‹¤.</p>

<h2 id="where-to-include-elmo">Where to include ELMo?</h2>

<p><img src="/assets/images/2020-01-02-Deep-contextualized-word-representations/01-01-2021-23.01.17.jpg" alt="Deep%20contextualized%20word%20representations%204e91492dc98641ada8f1fdfac764a546/01-01-2021-23.01.17.jpg" /></p>

<p>ìœ„ì—ì„œ ì–¸ê¸‰í–ˆë“¯ supervised NLP modelì— ELMoë¥¼ ì ìš©í•  ë•Œì—ëŠ” input layerì˜ ì§í›„ì— ELMoë¥¼ ì‚½ì…í–ˆë‹¤. SQuAD, SNLI, SRLì˜ baseline modelì€ ëª¨ë‘ biRNN modelì¸ë°, ELMoë¥¼ biRNN ì§í›„ì—ë„ ì‚½ì…ì„ í•œ ë’¤ ì„±ëŠ¥ì„ ë¹„êµí–ˆë‹¤. SQuADì™€ SNLIì— ìˆì–´ì„œëŠ” ELMoë¥¼ biRNN ì´í›„ì—ë„ ì¶”ê°€í•˜ëŠ” ê²ƒì´ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì—¬ì¤¬ëŠ”ë°, ì´ëŠ” SNLIì™€ SQuADëŠ” biRNN ì§í›„ attention layerê°€ ìˆëŠ”ë°, biRNNê³¼ attention layer ì‚¬ì´ì— ELMoë¥¼ ì¶”ê°€í•¨ìœ¼ë¡œì¨ ELMo representationì— attentionì´ ì§ì ‘ì ìœ¼ë¡œ ë°˜ì˜ëê¸° ë•Œë¬¸ì´ë¼ê³  ìœ ì¶”í•´ ë³¼ ìˆ˜ ìˆë‹¤.</p>

<h2 id="what-information-is-captured-by-the-bilms-representations">What information is captured by the biLMâ€™s representations?</h2>

<p><img src="/assets/images/2020-01-02-Deep-contextualized-word-representations/01-01-2021-23.22.09.jpg" alt="Deep%20contextualized%20word%20representations%204e91492dc98641ada8f1fdfac764a546/01-01-2021-23.22.09.jpg" /></p>

<h3 id="word-sense-disambiguation">Word sense disambiguation</h3>

<p><img src="/assets/images/2020-01-02-Deep-contextualized-word-representations/01-01-2021-23.22.19.jpg" alt="Deep%20contextualized%20word%20representations%204e91492dc98641ada8f1fdfac764a546/01-01-2021-23.22.19.jpg" /></p>

<p>WSDëŠ” ë‹¤ì˜ì–´ì˜ ì˜ë¯¸ë¥¼ êµ¬ë¶„ì§“ëŠ” taskë¡œ embeddingì´ ì–¼ë§ˆë‚˜ semantic ì •ë³´ë¥¼ ì˜ ë‹´ê³  ìˆëŠ”ì§€ì— ëŒ€í•œ ì§€í‘œì´ë‹¤. ELMoëŠ” WSD-specificí•œ modelê³¼ ë™ë“±í•œ ìˆ˜ì¹˜ë¥¼, CoVeë³´ë‹¤ëŠ” ì›”ë“±íˆ ë†’ì€ ìˆ˜ì¹˜ë¥¼ ë‹¬ì„±í–ˆë‹¤. ì£¼ëª©í• ë§Œí•œ ì ì€ ELMoì˜ first LSTM layerì˜ outputë³´ë‹¤ëŠ” second layer (top layer)ì˜ outputì´ WSDì—ì„œ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì˜€ë‹¤ëŠ” ì ì´ë‹¤.</p>

<h3 id="pos-tagging">POS tagging</h3>

<p><img src="/assets/images/2020-01-02-Deep-contextualized-word-representations/01-01-2021-23.22.28.jpg" alt="Deep%20contextualized%20word%20representations%204e91492dc98641ada8f1fdfac764a546/01-01-2021-23.22.28.jpg" /></p>

<p>POS taggingì€ wordì˜ í’ˆì‚¬ë¥¼ taggingí•˜ëŠ” taskë¡œ embeddingì´ ì–¼ë§ˆë‚˜ syntax ì •ë³´ë¥¼ ì˜ ë‹´ê³  ìˆëŠ”ì§€ì— ëŒ€í•œ ì§€í‘œì´ë‹¤. ì—¬ê¸°ì„œë„ ELMoëŠ” POS tagging-specific modelê³¼ ë™ë‘¥í•œ ìˆ˜ì¤€ì˜ ì„±ëŠ¥ì„, CoVeë³´ë‹¤ëŠ” ì›”ë“±íˆ ë†’ì€ ì„±ëŠ¥ì„ ë³´ì—¬ì¤¬ë‹¤. WSDì™€ëŠ” ë‹¤ë¥´ê²Œ ì˜¤íˆë ¤ first LSTM layerì˜ outputì´ top layerì˜ outputë³´ë‹¤ POS taggingì—ì„œ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì˜€ë‹¤ëŠ” ì ì´ ì£¼ëª©í•  ë§Œí•˜ë‹¤.</p>

<h3 id="implications-for-supervised-tasks">Implications for supervised tasks</h3>

<p>ê²°ë¡ ì ìœ¼ë¡œ ELMoì—ì„œ ê° layerëŠ” ë‹´ê³  ìˆëŠ” ì •ë³´ì˜ ì¢…ë¥˜ê°€ ë‹¤ë¥´ë‹¤ê³  í•  ìˆ˜ ìˆëŠ”ë°, ì¸µì´ ë‚®ì€ layer(input layerì— ê°€ê¹Œìš´ layer)ì¼ìˆ˜ë¡ syntax ì •ë³´ë¥¼, ì¸µì´ ë†’ì€ layer(output layerì— ê°€ê¹Œìš´ layer)ì¼ìˆ˜ë¡ semantic ì •ë³´ë¥¼  ì €ì¥í•œë‹¤.</p>

<h2 id="sample-efficiency">Sample efficiency</h2>

<p><img src="/assets/images/2020-01-02-Deep-contextualized-word-representations/01-01-2021-23.47.27.jpg" alt="Deep%20contextualized%20word%20representations%204e91492dc98641ada8f1fdfac764a546/01-01-2021-23.47.27.jpg" /></p>

<p>ELMoì˜ ì‚¬ìš©ì€ ì¼ì • ìˆ˜ì¤€ ì´ìƒì˜ ì„±ëŠ¥ ë‹¬ì„±ì— í•„ìš”í•œ parameter update íšŸìˆ˜ ë° ì „ì²´ training set sizeë¥¼ íšê¸°ì ìœ¼ë¡œ ì¤„ì—¬ì¤€ë‹¤. SRL taskì— ìˆì–´ì„œ ELMo ì‚¬ìš© ì´ì „ baseline modelì˜ ê²½ìš°ì—ëŠ” 486 epochê°€ ì§€ë‚˜ì„œì•¼ scoreê°€ ìˆ˜ë ´í–ˆëŠ”ë°, ELMoë¥¼ ì¶”ê°€í•˜ê³  ë‚œ ë’¤ì—ëŠ” 10 epochë§Œì— baseline modelì˜ scoreë¥¼ ëŠ¥ê°€í–ˆë‹¤.</p>

<p>Figure 1ì—ì„œëŠ” ê°™ì€ í¬ê¸°ì˜ datasetì—ì„œ ELMoë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš°ê°€ í›¨ì”¬ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë‚¸ë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì¤€ë‹¤. ì‹¬ì§€ì–´ SRL taskì—ì„œëŠ” ELMoë¥¼ ì‚¬ìš©í•œ modelì´ training datasetì˜ ë‹¨ 1%ì„ í•™ìŠµí–ˆì„ ë•Œ ë‹¬ì„±í•œ ìˆ˜ì¹˜ì™€ baseline modelì´ training datasetì˜ 10%ë¥¼ í•™ìŠµí–ˆì„ ë•Œì˜ ìˆ˜ì¹˜ê°€ ë™ì¼í•˜ë‹¤ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.</p>

<h2 id="visualization-of-learned-weights">Visualization of learned weights</h2>

<p><img src="/assets/images/2020-01-02-Deep-contextualized-word-representations/01-01-2021-23.47.33.jpg" alt="Deep%20contextualized%20word%20representations%204e91492dc98641ada8f1fdfac764a546/01-01-2021-23.47.33.jpg" /></p>

<p>softmax-normalized parameter \(s_j\)ë¥¼ ì‹œê°í™”í•œ ê²ƒì´ë‹¤. ELMoë¥¼ biRNNì˜ inputê³¼ outputì— ì‚¬ìš©í–ˆì„ ë•Œë¥¼ ê°ê° ë‚˜ëˆ  ë¹„êµí–ˆë‹¤. ELMoê°€ inputì— ì‚¬ìš©ëœ ê²½ìš°ì—ëŠ” ëŒ€ê°œ first LSTM layerê°€ ì„ í˜¸ë˜ëŠ” ê²½í–¥ì„ ë³´ì˜€ë‹¤. íŠ¹íˆë‚˜ SQuADì—ì„œ ì´ëŸ¬í•œ ê²½í–¥ì„±ì´ ê°€ì¥ ë‘ë“œëŸ¬ì§€ê²Œ ë‚˜íƒ€ë‚¬ë‹¤. ë°˜ë©´ ELMOê°€ outputì— ì‚¬ìš©ëœ ê²½ìš°ì—ëŠ” weightê°€ ê· í˜•ìˆê²Œ ë¶„ë°°ë˜ì—ˆì§€ë§Œ ë‚®ì€ layerê°€ ì¡°ê¸ˆ ë” ë†’ì€ ì„ í˜¸ë¥¼ ë³´ì˜€ë‹¤.</p>

<h1 id="conclusion">Conclusion</h1>

<p>biLMì„ ì‚¬ìš©í•´ ë†’ì€ ìˆ˜ì¤€ì˜ contextë¥¼ í•™ìŠµí•˜ëŠ” ELMo modelì„ ì œì•ˆí–ˆë‹¤. ELMo modelì„ ì‚¬ìš©í•˜ë©´ ëŒ€ë¶€ë¶„ì˜ NLP taskì—ì„œ ì„±ëŠ¥ì´ í–¥ìƒë˜ì—ˆë‹¤. ë˜í•œ layerì˜ ì¸µì´ ì˜¬ë¼ê°ˆìˆ˜ë¡ syntaxë³´ë‹¤ëŠ” semanticí•œ ì •ë³´ë¥¼ ë‹´ì•„ë‚¸ë‹¤ëŠ” ì‚¬ì‹¤ë„ ë°œê²¬í•´ëƒˆë‹¤. ë•Œë¬¸ì— ì–´ëŠ í•œ layerë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒë³´ë‹¤ëŠ” ëª¨ë“  layerì˜ representationì„ ê²°í•©í•´ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì „ë°˜ì ì¸ ì„±ëŠ¥ í–¥ìƒì— ë„ì›€ì´ ëœë‹¤ëŠ” ê²°ë¡ ì„ ë‚´ë¦´ ìˆ˜ ìˆë‹¤.</p>
:ET