I"¶I<h1 id="mass-masked-sequence-to-sequence-pre-training-for-language-generation">MASS: Masked Sequence to Sequence Pre-training for Language Generation</h1>
<p>title: MASS: Masked Sequence to Sequence Pre-training for Language Generation
subtitle: MASS
categories: Paper Review
tags: NLP
date: 2021-01-19 12:59:35 +0000
last_modified_at: 2021-01-19 12:59:35 +0000
â€”</p>

<p>Archive Link: https://arxiv.org/abs/1905.02450
Created: Sep 21, 2020 3:19 PM
Field: NLP
Paper Link: https://arxiv.org/pdf/1905.02450.pdf
Status: not checked
Submit Date: Jun 21, 2019</p>

<h1 id="abstract">Abstract</h1>

<p>BERTì—ì„œ ì˜ê°ì„ ë°›ì•„ Pre-training / fine-tuning, encoder/decoderë¥¼ ì±„íƒí•œ MAsked Sequence to Sequence (MASS) modelì„ ë§Œë“¤ì–´ëƒˆë‹¤. randomí•˜ê²Œ input sentenceì— ì—°ì†ì ì¸ maskë¥¼ ë¶€ì—¬í•œ ë’¤ decoderê°€ predictí•˜ëŠ” ë°©ì‹ìœ¼ë¡œ encoderì™€ decoderë¥¼ Pre-trainingì‹œì¼œ Language Generation Taskì— ì í•©í•œ Modelì„ ê°œë°œí–ˆë‹¤. íŠ¹íˆ datasetì´ ì ì€ Language Generation taskì—ì„œ ë¹„ì•½ì ì¸ ì„±ëŠ¥ í–¥ìƒì´ ìˆì—ˆë‹¤.</p>

<h1 id="introduction">Introduction</h1>

<p>Pre-trainingì€ target taskì— ëŒ€í•œ labeled data(pair data)ê°€ ì ìœ¼ë©´ì„œ í•´ë‹¹ languageì— ëŒ€í•œ data(unpaired data)ëŠ” ë§ì„ ë•Œì— ê°€ì¥ ì í•©í•˜ë‹¤ê³  í•  ìˆ˜ ìˆë‹¤. BERTëŠ” language understandingì„ ëª©í‘œë¡œ í•˜ëŠ” ë°©ì‹ì´ê¸°ì— language generation taskì—ëŠ” ì í•©í•˜ì§€ ì•Šë‹¤. MASSì˜ BERTì™€ Masking ruleì—ì„œ ë‹¤ìŒê³¼ ê°™ì€ ì°¨ì´ì ì„ ë‘”ë‹¤. ì²«ë²ˆì§¸ë¡œ, MASK tokenì´ ì—°ì†ì ìœ¼ë¡œ ë°°ì¹˜ë˜ê³ , í•´ë‹¹ MASK Tokenì„ ì˜ˆì¸¡í•˜ë©´ì„œ encoderëŠ” unmasked tokenë“¤ì˜ contextë¥¼ í•™ìŠµí•  ìˆ˜ ìˆë„ë¡ í•œë‹¤. ë‘ë²ˆì§¸ë¡œ decoderì˜ target tokenì—ë„ MASKë¥¼ ë¶€ì—¬í•¨ìœ¼ë¡œì¨ predict ì‹œì— encoderë¥¼ ë” ë§ì´ í™œìš©í•  ìˆ˜ ìˆë„ë¡ í•œë‹¤.</p>

<h1 id="mass">MASS</h1>

<h2 id="sequence-to-sequence-learning">Sequence to Sequence Learning</h2>

<p>source sentenceë¥¼ \(x\), target sentenceë¥¼ \(y\)ë¼ê³  í•œë‹¤. ê°ê° domain \(X\)ì™€ \(Y\)ì— ì†í•œë‹¤. sentence pairë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜í•  ìˆ˜ ìˆë‹¤.</p>

\[\left(x,y\right) \in \left(X,Y\right), \\x= \left(x_1,x_2, ..., x_m\right)\\y = \left(y_1,y_2, ..., y_n\right)\]

<p>Objective Functionì€ ë‹¤ìŒê³¼ ê°™ë‹¤. domain \(X\)ì™€ \(Y\)ì— ëŒ€í•œ ëª¨ë“  sentence pairë“¤ì— ëŒ€í•´ \(x\)ê°€ ì£¼ì–´ì¡Œì„ ë•Œ \(y\)ë¥¼ êµ¬í•˜ëŠ” ì¡°ê±´ë¶€ í™•ë¥ ì˜ log liklihoodë¥¼ ë”í•œ ê²ƒì´ë‹¤.</p>

\[L(\theta;(X,Y)) = \sum_{\left(x,y\right)\in\left(X,Y\right)} {log P\left(y|x;\theta\right)}\]

<p>ì¡°ê±´ë¶€ í™•ë¥ ì„ êµ¬í•˜ëŠ” ìˆ˜ì‹ì€ ë‹¤ìŒê³¼ ê°™ë‹¤. source sentence ì „ì²´ì™€ target sentenceì—ì„œ í˜„ì¬ token ì´ì „ì˜ ëª¨ë“  tokenë“¤ì´ ì£¼ì–´ì¡Œì„ ë•Œ í˜„ì¬ tokenì— ëŒ€í•œ ì¡°ê±´ë¶€ í™•ë¥ ì´ë‹¤.</p>

\[P\left(y|x;\theta\right)=\prod_{t=1}^n{P\left(y_t|y_{&lt;t},x;\theta\right)}\]

<p>\(y_{&lt;t}\)ëŠ”  \(y_1\sim y_{t-1}\)ì˜ tokenë“¤ì´ë‹¤.</p>

<h2 id="masked-sequence-to-sequence-pre-training">Masked Sequence to Sequence Pre-training</h2>

<p>MASSëŠ” BERTì™€ ë‹¬ë¦¬ MASK tokenì´ discreteí•˜ê²Œ ë¶„í¬ë˜ì–´ ìˆì§€ ì•Šê³  ì—°ì†ì ìœ¼ë¡œ ë­‰ì³ì ¸ ìˆë‹¤. ì´ì— ë”°ë¼ ìƒˆë¡œìš´ parameter \(k\)ê°€ ë“±ì¥í•œë‹¤. \(k\)ëŠ” MASK tokenì˜ ê°œìˆ˜ì¸ë°, \(k\)ê°œì˜ MASK tokenì€ ì—°ì†ë˜ì–´ ìˆë‹¤. MASK tokenì´ \(u\)ë¶€í„° \(v\)ê¹Œì§€ ë¶„í¬ë˜ì–´ ìˆë‹¤ë©´ \(0&lt;u&lt;v&lt;m\) (\(m\)ì€ ì „ì²´ sentence ê¸¸ì´)ì´ê³ , \(k = v - u + 1\)ì´ë‹¤. Pre-trainingì—ì„œ ì‚¬ìš©í•˜ëŠ” Objective Functionì€ ë‹¤ìŒê³¼ ê°™ë‹¤. ì¡°ê±´ë¶€ í™•ë¥ ì˜ ì¡°ê±´ìœ¼ë¡œ ë‹¤ìŒì˜ 2ê°€ì§€ ê°’ì´ ì£¼ì–´ì§€ê²Œ ëœë‹¤.</p>

<ol>
  <li>MASKê°€ ì”Œì›Œì§„ input sentence ì „ì²´</li>
  <li>input sentenceì—ì„œ MASKê°€ ì”Œì›Œì§„ tokenë“¤ ì¤‘ í˜„ì¬ token \(x_t\) ì´ì „ì˜ tokenë“¤ì˜ MASK ì”Œì›Œì§€ê¸° ì´ì „ ì›ë³¸ token</li>
</ol>

\[L(\theta;X)=\frac{1}{\vert X\vert}\sum_{x\in X}log\ P\left(x^{u:v}|x^{\backslash u:v};\theta\right)\\=\frac{1}{\vert X\vert}\sum_{x\in X}log\prod_{t=u}^vP\left(x_t^{u:v}|x_{&lt;t}^{u:v},x^{\backslash u:v};\theta\right)\]

<p>\(x^{u:v}\)ëŠ” sentence \(x\)ì—ì„œ \(u\)ë¶€í„° \(v\)ê¹Œì§€ì˜ tokensë¥¼ ëœ»í•˜ê³ , \(x^{\backslash u:v}\)ëŠ” \(u\)ë¶€í„° \(v\)ê¹Œì§€ MASKëœ sentence \(x\) ì „ì²´ë¥¼ ëœ»í•œë‹¤.</p>

<p><img src="/assets/images/2021-01-19-MASS-Masked-Sequence-to-Sequence-Pre-training-for-Language-Generation/09-04-2020-14.43.42.jpg" alt="MASS%20Masked%20Sequence%20to%20Sequence%20Pre-training%20for%20%2071c3529aece54ebab2fa54feb4adda92/09-04-2020-14.43.42.jpg" /></p>

<p>êµ¬ì²´ì ì¸ ì˜ˆì‹œë¥¼ ì‚´í´ë³´ì. ìœ„ì˜ figureëŠ” \(x_3, x_4, x_5, x_6\)ì´ maskingëœ ìƒí™©ì´ë‹¤. \(k=4\)ì´ê³ , \(u=3, v=6\)ì´ë‹¤. Encoderì˜ input ìœ¼ë¡œëŠ” maskingëœ input sentence \(x^{\backslash u:v}\)ê°€ ë“¤ì–´ì˜¤ê²Œ ë˜ëŠ”ë°, ì´ ê²½ìš°ì—ëŠ” \(x^{\backslash 3:6}\)ì´ë‹¤. Attention ê¸°ë²•ì„ ì ìš©í•´ Decoderë¡œ ê°’ì´ ë„˜ì–´ì˜¤ê³ , Decoderì—ì„œëŠ” ìƒˆë¡œìš´ inputìœ¼ë¡œ \(x^{u:v}\), ì´ ê²½ìš°ì—ëŠ” \(x^{3:6}\)ì„ ì…ë ¥ìœ¼ë¡œ ë°›ëŠ”ë‹¤. ì´ ë•Œ input sentenceì—ì„œ maskingì´ ë˜ì§€ ì•Šì€ tokenë“¤ (\(x_1, x_2, x_7,x_8)\)ì˜ ê²½ìš°ì—ëŠ” Decoderì— inputìœ¼ë¡œ ë“¤ì–´ì˜¤ì§€ ì•ŠëŠ”ë‹¤. Decoderì˜ inputìœ¼ë¡œ ë“¤ì–´ì˜¨ tokenë“¤ \(x_3, x_4, x_5, x_6\) ì¤‘ ì‹¤ì œë¡œëŠ” \(x^{u:v}_{&lt;t}\)ë¡œ ì‚¬ìš©ë˜ê¸° ë•Œë¬¸ì— ë§ˆì§€ë§‰ token \(x_6\)ì€ ì‚¬ìš©ë˜ì§€ ì•ŠëŠ”ë‹¤.</p>

<h2 id="discussions">Discussions</h2>

<h3 id="special-case--k1-km">Special Case ( k=1, k=m)</h3>

<p>MASSì—ì„œ hyperparameter \(k\)ëŠ” ë§¤ìš° ì¤‘ìš”í•œ parameterì´ë‹¤. \(k\)ê°€ íŠ¹ìˆ˜í•œ ê°’ì¼ ë•Œì— ëŒ€í•´ì„œ ì‚´í´ë³´ì.</p>

<p>\(k=1\)ì¸ ê²½ìš°ëŠ” ì‚¬ì‹¤ BERTì—ì„œì˜ MLM(Masked Langage Model)ì´ë‹¤. BERTì˜ MLMì— ëŒ€í•œ ìì„¸í•œ ì„¤ëª…ì€ ì•„ë˜ë¥¼ ì°¸ì¡°í•˜ì.</p>

<p><a href="https://www.notion.so/Copy-of-BERT-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding-5cb659c4a2164cfa8ffc5dadfc411993">Copy of BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a></p>

<p><img src="/assets/images/2021-01-19-MASS-Masked-Sequence-to-Sequence-Pre-training-for-Language-Generation/09-04-2020-15.15.36.jpg" alt="MASS%20Masked%20Sequence%20to%20Sequence%20Pre-training%20for%20%2071c3529aece54ebab2fa54feb4adda92/09-04-2020-15.15.36.jpg" /></p>

<p>BERTì˜ MLMì€ MASK tokenì— ëŒ€í•´ì„œ predictë§Œ í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ Pre-trainingì„ ìˆ˜í–‰í–ˆë‹¤. ì¦‰ Decoderì— ì–´ë– í•œ inputë„ ì¶”ê°€ì ìœ¼ë¡œ ì£¼ì–´ì§€ì§€ ì•Šê³ , Encoderì—ì„œ ë„˜ì–´ì˜¨ Context Vectorë§Œì„ ì‚¬ìš©í•´ MASK tokenì„ predictí•˜ëŠ” trainingì´ë‹¤. ì´ëŠ” MASSì—ì„œ \(k=1\)ì¼ ë•Œì˜ ê²½ìš°ì´ë‹¤.</p>

<p>í•œí¸, \(k=m\) (\(m\)ì€ sentenceì˜ token ê°œìˆ˜)ì¸ ê²½ìš°ëŠ” ì¼ë°˜ì ì¸ Language Generation Modelì´ë‹¤.</p>

<p><img src="/assets/images/2021-01-19-MASS-Masked-Sequence-to-Sequence-Pre-training-for-Language-Generation/09-04-2020-15.15.46.jpg" alt="MASS%20Masked%20Sequence%20to%20Sequence%20Pre-training%20for%20%2071c3529aece54ebab2fa54feb4adda92/09-04-2020-15.15.46.jpg" /></p>

<p>\(k=m\)ì¸ ê²½ìš°ëŠ” ì‚¬ì‹¤ ì¼ë°˜ì ì¸ Language Modelì˜ ê²½ìš°ì´ë‹¤. \(k=m\)ë¼ëŠ” ê²ƒì€ ë‹¤ì‹œ ë§í•´ input sentenceì˜ ëª¨ë“  tokenì´ maskingë˜ì—ˆë‹¤ëŠ” ì˜ë¯¸ì´ê³ , ì´ëŠ” Encoderì˜ inputìœ¼ë¡œ ì•„ë¬´ ê°’ë„ ë“¤ì–´ì˜¤ì§€ ì•ŠëŠ” ê²½ìš°ì™€ ê°™ë‹¤. í•œí¸ Decoderì˜ ì…ì¥ì—ì„œëŠ” inputìœ¼ë¡œ original sentenceì˜ masked tokenë“¤ì´ ë“¤ì–´ì˜¤ê²Œ ë˜ëŠ”ë°, original sentenceëŠ” ëª¨ë‘ maskingë˜ì—ˆìœ¼ë¯€ë¡œ ëª¨ë“  tokenì´ Decoderë¡œ ë“¤ì–´ì˜¤ëŠ” ê²½ìš°ì´ë‹¤. ì´ëŠ” ê²°êµ­ Encoderê°€ ì—†ì´ Decoderë§Œ ì‘ë™í•˜ëŠ” ìƒí™©ì´ë¼ê³  ë³¼ ìˆ˜ ìˆë‹¤. ì¼ë°˜ì ì¸ GPT modelì™€ ê°™ë‹¤.</p>

<p>ìœ„ì˜ ë‘ ê°€ì§€ special caseì™€ ì¼ë°˜ì ì¸ caseë¥¼ Tableë¡œ ì •ë¦¬í•˜ë©´ ì•„ë˜ì™€ ê°™ë‹¤.</p>

<p><img src="/assets/images/2021-01-19-MASS-Masked-Sequence-to-Sequence-Pre-training-for-Language-Generation/09-04-2020-15.07.10.jpg" alt="MASS%20Masked%20Sequence%20to%20Sequence%20Pre-training%20for%20%2071c3529aece54ebab2fa54feb4adda92/09-04-2020-15.07.10.jpg" /></p>

<h3 id="comparison-with-existing-model">Comparison with existing model</h3>

<ol>
  <li>MASK tokenë§Œ predictí•˜ê²Œ í•¨ìœ¼ë¡œì¨ EncoderëŠ” unmasked tokenë“¤ì— ëŒ€í•œ contextë¥¼ í•™ìŠµí•˜ê²Œ ë˜ê³ , decoderê°€ encoderë¡œë¶€í„° ë” ì¢‹ì€ ì •ë³´ë¥¼ ê°€ì ¸ê°ˆ ìˆ˜ ìˆë„ë¡ í•œë‹¤. (encoderê°€ context vectorë¥¼ ì œëŒ€ë¡œ ìƒì„±í•´ë‚´ë„ë¡ í•œë‹¤.)</li>
  <li>MASK tokenì„ ì—°ì†ì ìœ¼ë¡œ ë°°ì¹˜í•¨ìœ¼ë¡œì¨ Decoderê°€ ë‹¨ìˆœ wordë“¤ì´ ì•„ë‹Œ subsentenceë¥¼ ë§Œë“¤ì–´ë‚¼ ìˆ˜ ìˆë„ë¡ í•œë‹¤. (better language modeling capability)</li>
  <li>Decoderì˜ inputìœ¼ë¡œ source sentenceì˜ unmasked tokenë“¤ì´ ë“¤ì–´ì˜¤ì§€ ëª»í•˜ê²Œ í•¨ìœ¼ë¡œì¨ Decoderì˜ input tokenë“¤ì—ì„œ ì •ë³´ë¥¼ í™œìš©í•˜ê¸°ë³´ë‹¤ Encoderì—ì„œ ë„˜ì–´ì˜¨ Context Vectorì˜ ì •ë³´ë¥¼ í™œìš©í•  ìˆ˜ ìˆë„ë¡ í–ˆë‹¤.</li>
</ol>

<h1 id="experiments-and-results">Experiments and Results</h1>

<h2 id="mass-pre-training">MASS Pre-training</h2>

<h3 id="model-configuration">Model Configuration</h3>

<p>6ê°œì˜ encoder, decoder layerë¥¼ ê°€ì§„ Transformerë¥¼ Base Modelë¡œ ì„ íƒí–ˆë‹¤. NMTë¥¼ ìœ„í•´ source languageì™€ target languageì— ëŒ€í•´ ê°ê° monolingual dataë¡œ pre-trainì„ ì§„í–‰í–ˆë‹¤. English-French, English-German, English-Romanianì˜ 3ê°€ì§€ pairë¥¼ ì‚¬ìš©í–ˆë‹¤. ê° pairì— ëŒ€í•´ì„œ ë³„ê°œë¡œ í•™ìŠµì„ ì§„í–‰í–ˆìœ¼ë©°, ì´ ë•Œ source languageì™€ target languageë¥¼ êµ¬ë¶„í•˜ê¸° ìœ„í•´ ìƒˆë¡œìš´ language embeddingì„ encoder inputê³¼ decoder inputì— ì¶”ê°€í–ˆë‹¤. Text Summarizationê³¼ Conversational Response Generation taskì— ëŒ€í•´ì„œëŠ” ëª¨ë‘ Englishì— ëŒ€í•´ì„œë§Œ pre-trainì„ ì§„í–‰í–ˆë‹¤.</p>

<h3 id="datasets">Datasets</h3>

<p>WMT News Crawl Datasetì„ ì‚¬ìš©í–ˆë‹¤. English, French, German, Romanianì— ëŒ€í•´ì„œ Pre-trainì„ ì§„í–‰í–ˆë‹¤. ì´ ì¤‘ Romanianì˜ ê²½ìš°ì—ëŠ” dataê°€ ì ì€ languageì´ë‹¤. low-resource languageì— ëŒ€í•œ MASSì˜ pre-training ì„±ëŠ¥ì„ ì¸¡ì •í•˜ê¸° ìœ„í•´ ì±„íƒí–ˆë‹¤. ëª¨ë“  languageì— ëŒ€í•´ BPEë¥¼ ì ìš©í–ˆë‹¤.</p>

<h3 id="pre-training-details">Pre-Training Details</h3>

<p>BERTì™€ ë™ì¼í•œ masking ruleì„ ì±„íƒí–ˆë‹¤.MASK tokenìœ¼ë¡œ ë³€ê²½ë˜ëŠ” token ì¤‘ ì‹¤ì œë¡œ ë³€ê²½ë˜ëŠ” tokenì€ 80%ì´ê³ , ë‹¤ë¥¸ randomí•œ tokenìœ¼ë¡œ ë³€ê²½ë˜ëŠ” ê²ƒì´ 10%, ë³€ê²½ë˜ì§€ ì•ŠëŠ” ê²ƒì´ 10%ì´ë‹¤. hyperparameter \(k\)ëŠ” ì „ì²´ sentence ê¸¸ì´ \(m\)ì˜ 50%ì™€ ë¹„ìŠ·í•œ ìˆ˜ì¹˜ê°€ ë˜ë„ë¡ ì„¤ì •í–ˆë‹¤. decoderì˜ inputìœ¼ë¡œ ë“¤ì–´ì˜¤ëŠ” sentenceì— ëŒ€í•´ì„œëŠ” ê¸°ì¡´ì˜ original sentenceì—ì„œì˜ positional encodingì€ ìˆ˜ì •ë˜ì§€ ì•ŠëŠ”ë‹¤. Adam Optimizerë¥¼ ì‚¬ìš©í–ˆê³ , lrì€ 0.0001ì´ë©°, batch_sizeëŠ” 3000ì´ë‹¤.</p>

<p>fine-tuningì„ ìˆ˜í–‰í•  datasetì´ ì ì€ ê²½ìš°(paired sentenceê°€ ì ì€ ê²½ìš°)ì— ëŒ€í•´ì„œë„ ì„±ëŠ¥ì„ ì¸¡ì •í•œë‹¤. íŠ¹íˆ ì•„ì˜ˆ fine-tuning dataê°€ ì—†ëŠ” ìƒíƒœì—ì„œë„ NMTë¥¼ ì˜ ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ”ì§€ì— ëŒ€í•´ì„œ ì‚´í´ë³¸ë‹¤.</p>

<h2 id="fine-tuning-on-nmt">Fine-Tuning on NMT</h2>

<h3 id="experimental-setting">Experimental Setting</h3>

<p>Unsupervised NMTë¥¼ ìˆ˜í–‰í•˜ê¸° ìœ„í•´ì„œ back-translationì„ ì‚¬ìš©í•œë‹¤. bilingual dataê°€ ì—†ê¸° ë•Œë¬¸ì— soruce language dataë¡œ ê°€ìƒì˜ bilingual dataë¥¼ ìƒì„±í•´ë‚´ëŠ” ê²ƒì´ë‹¤. Pre-Trainingê³¼ ë™ì¼í•˜ê²Œ Adam Optimizer, lr=0.0001ì„ ì±„íƒí–ˆìœ¼ë©°, batck_sizeëŠ” 2000ì´ë‹¤. evaluationì„ ìœ„í•´ BLEU Scoreë¥¼ ì‚¬ìš©í–ˆë‹¤.</p>

<h3 id="results-on-unsupervised-nmt">Results on Unsupervised NMT</h3>

<p><img src="/assets/images/2021-01-19-MASS-Masked-Sequence-to-Sequence-Pre-training-for-Language-Generation/09-04-2020-16.53.36.jpg" alt="MASS%20Masked%20Sequence%20to%20Sequence%20Pre-training%20for%20%2071c3529aece54ebab2fa54feb4adda92/09-04-2020-16.53.36.jpg" /></p>

<p>RNN ê³„ì—´ì˜ Modelë“¤(1,2í–‰)ê³¼ Pre-train ë°©ì‹ì´ ì•„ë‹Œ Transformer Model(3,4í–‰), Pre-train Transfor Model(5í–‰)ë“¤ì„ ëª¨ë‘ ëŠ¥ê°€í–ˆë‹¤. Unsupervised NMTëŠ” ë‚œì œì´ê¸°ì— ì ˆëŒ€ì ì¸ ScoreëŠ” ë‚®ì§€ë§Œ, ê¸°ì¡´ì˜ SOTA Modelì¸ XLMì„ ëŠ¥ê°€í–ˆë‹¤ëŠ” ì ì—ì„œ ì˜ë¯¸ê°€ ìˆë‹¤.</p>

<h3 id="compared-with-other-pre-training-methods">Compared with Other Pre-training Methods</h3>

<p><img src="/assets/images/2021-01-19-MASS-Masked-Sequence-to-Sequence-Pre-training-for-Language-Generation/09-04-2020-16.59.37.jpg" alt="MASS%20Masked%20Sequence%20to%20Sequence%20Pre-training%20for%20%2071c3529aece54ebab2fa54feb4adda92/09-04-2020-16.59.37.jpg" /></p>

<p>ë‹¤ì–‘í•œ Pre-train Methodsë¥¼ ì ìš©í•œ Modelë“¤ê³¼ Unsupervised NMTì—ì„œì˜ BLEU Scoreë¥¼ ë¹„êµí•´ë³¸ë‹¤. BERTì™€ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ Pre-trainì„ ì§„í–‰í•œ BERT+LM Model, denoising auto-encoder Pre-train ë°©ì‹ì„ ì ìš©í•œ DAEë¥¼ ëª¨ë‘ ëŠ¥ê°€í–ˆë‹¤.</p>

<h3 id="experiments-on-low-resource-nmt">Experiments on Low-Resource NMT</h3>

<p><img src="/assets/images/2021-01-19-MASS-Masked-Sequence-to-Sequence-Pre-training-for-Language-Generation/09-04-2020-17.04.30.jpg" alt="MASS%20Masked%20Sequence%20to%20Sequence%20Pre-training%20for%20%2071c3529aece54ebab2fa54feb4adda92/09-04-2020-17.04.30.jpg" /></p>

<p>Pre-trainì€ 20000 step ì§„í–‰í–ˆìœ¼ë©°, bilingual datasetì˜ sample í¬ê¸°ê°€ 10K, 100K, 1Mì¸ ê²½ìš°ì— ëŒ€í•´ì„œ ê°ê°ì˜ ì–¸ì–´ì— ëŒ€í•´ ë³„ê°œë¡œ ì„±ëŠ¥ì„ ì¸¡ì •í–ˆë‹¤. baseline modelì€ pre-train ê³¼ì •ì´ ì—†ëŠ” modelì´ë‹¤. ëª¨ë“  ê²½ìš°ì— ìˆì–´ì„œ MASSê°€ baseline modelì„ ì••ë„í–ˆìœ¼ë©°, íŠ¹íˆë‚˜ Sampleì˜ í¬ê¸°ê°€ ì‘ì„ ìˆ˜ë¡(fine-tuningì„ ì ê²Œ ìˆ˜í–‰í• ìˆ˜ë¡) ì„±ëŠ¥ì˜ ì°¨ì´ê°€ ì»¸ë‹¤.</p>

<h2 id="fine-tuning-on-text-summarization">Fine-Tuning on Text Summarization</h2>

<h3 id="experiment-setting">Experiment Setting</h3>

<p>Gigaword corpusë¥¼ fine-tuning dataë¡œ ì‚¬ìš©í–ˆë‹¤. sample sizeê°€ 10K, 100K, 1M, 3.8Mì¸ ê²½ìš°ì— ëŒ€í•´ì„œ ë³„ê°œë¡œ ì„±ëŠ¥ì„ ì¸¡ì •í–ˆìœ¼ë©°, encoderì˜ inputì€ articleë¡œ, decoderì˜ outputì€ titleë¡œ ì„¤ì •í–ˆë‹¤.ì„±ëŠ¥ ì¸¡ì •ì€ ROUGE-1, ROUGE-2, ROUGE-Lì— ëŒ€í•œ F1 scoreë¡œ ì¸¡ì •í–ˆë‹¤. beam size=5ì¸ beam searchë¥¼ ì‚¬ìš©í–ˆë‹¤.</p>

<h3 id="results">Results</h3>

<p><img src="/assets/images/2021-01-19-MASS-Masked-Sequence-to-Sequence-Pre-training-for-Language-Generation/09-04-2020-17.12.48.jpg" alt="MASS%20Masked%20Sequence%20to%20Sequence%20Pre-training%20for%20%2071c3529aece54ebab2fa54feb4adda92/09-04-2020-17.12.48.jpg" /></p>

<p>pre-trainingì„ ìˆ˜í–‰í•˜ì§€ ì•Šì€ basemodelê³¼ ë¹„êµë¥¼ ìˆ˜í–‰í–ˆìœ¼ë©°, datasetì´ ì ì€ ê²½ìš°ì— ëŒ€í•´ì„œ ì••ë„ì ì¸ ì„±ëŠ¥ ê²©ì°¨ë¥¼ ë³´ì˜€ë‹¤ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.</p>

<h3 id="compared-with-other-pre-training-methods-1">Compared with Other Pre-Training Methods</h3>

<p><img src="/assets/images/2021-01-19-MASS-Masked-Sequence-to-Sequence-Pre-training-for-Language-Generation/09-04-2020-17.14.56.jpg" alt="MASS%20Masked%20Sequence%20to%20Sequence%20Pre-training%20for%20%2071c3529aece54ebab2fa54feb4adda92/09-04-2020-17.14.56.jpg" /></p>

<p>ë‹¤ë¥¸ Pre-training modelì— ëŒ€í•´ì„œë„ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì˜€ë‹¤.</p>

<h2 id="fine-tuning-on-conversational-response-generation">FIne-Tuning on Conversational Response Generation</h2>

<h3 id="experimental-setting-1">Experimental Setting</h3>

<p>Cornell movie dialog corpusë¥¼ Datasetìœ¼ë¡œ ì‚¬ìš©í–ˆë‹¤. ì´ 140Kì˜ pair ì¤‘ì—ì„œ 10KëŠ” validation set, 20KëŠ” test set, ë‚˜ë¨¸ì§€ëŠ” ëª¨ë‘ training setìœ¼ë¡œ ì‚¬ìš©í–ˆë‹¤. Perplexity(PPL)ì„ ì„±ëŠ¥ ì¸¡ì • ë‹¨ìœ„ë¡œ ì‚¬ìš©í–ˆë‹¤.</p>

<h3 id="results-1">Results</h3>

<p><img src="/assets/images/2021-01-19-MASS-Masked-Sequence-to-Sequence-Pre-training-for-Language-Generation/09-04-2020-17.18.54.jpg" alt="MASS%20Masked%20Sequence%20to%20Sequence%20Pre-training%20for%20%2071c3529aece54ebab2fa54feb4adda92/09-04-2020-17.18.54.jpg" /></p>

<p>Sample í¬ê¸°ê°€ 10K, 110Kì¸ ê²½ìš°ì— ëŒ€í•´ì„œ ì„±ëŠ¥ì„ ì¸¡ì •í–ˆë‹¤. MASSëŠ” ëª¨ë“  ê²½ìš°ì—ì„œ Pre-trainingì„ ìˆ˜í–‰í•˜ì§€ ì•Šì€ Baseline Modelê³¼, Pre-trainingì„ ìˆ˜í–‰í•œ BERT Modelë³´ë‹¤ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì˜€ë‹¤. PPLì€ ë” ë‚®ì€ Scoreê°€ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ëœ»í•œë‹¤.</p>

<h2 id="analysis-of-mass">Analysis of MASS</h2>

<h3 id="study-of-different-k">Study of Different k</h3>

<p><img src="/assets/images/2021-01-19-MASS-Masked-Sequence-to-Sequence-Pre-training-for-Language-Generation/09-04-2020-17.25.48.jpg" alt="MASS%20Masked%20Sequence%20to%20Sequence%20Pre-training%20for%20%2071c3529aece54ebab2fa54feb4adda92/09-04-2020-17.25.48.jpg" /></p>

<p>hyperparameter \(k\)ì— ëŒ€í•´ì„œ ìì„¸íˆ ì‚´í´ë³´ì. \(k\)ì˜ ê°’ ë³€í™”ì— ë”°ë¥¸ Scoreë“¤ì„ ì¸¡ì •í•´ë³¸ë‹¤.</p>

<p>(a)ì™€ (b)ëŠ” ê°ê° English, Frenchì— ëŒ€í•´ Pre-trainingì„ ì‹œí‚¨ ì§í›„(fine-tuning ì—†ì´)ì˜ PPL Scoreë¥¼ ë‚˜íƒ€ë‚¸ ê²ƒì´ë‹¤. \(k\)ê°€ \(m\)ì˜ 50%~70% ì¸ êµ¬ê°„ì—ì„œ ê°€ì¥ ì¢‹ì€ ìˆ˜ì¹˜ë¥¼ ë³´ì˜€ë‹¤.</p>

<p>(c)ëŠ” English-French NMTì— ëŒ€í•œ BLEU Scoreì´ë‹¤. (d)ëŠ” Text Summarizationì— ëŒ€í•œ ROUGUE scoreì´ë‹¤. (e)ëŠ” Conversational Response Generationì— ëŒ€í•œ PPL Scoreì´ë‹¤. ëª¨ë‘ ê³µí†µì ìœ¼ë¡œ \(k\)ê°€ \(m\)ì˜ 50%ì¸ êµ¬ê°„ì—ì„œ ê°€ì¥ ì¢‹ì€ ìˆ˜ì¹˜ë¥¼ ë³´ì˜€ë‹¤.</p>

<p>\(k\)ê°€ \(m\)ì˜ 50%ë¼ëŠ” ìˆ˜ì¹˜ëŠ” ì§ê´€ì ìœ¼ë¡œ ì´í•´í–ˆì„ ë•Œì—ë„ ê°€ì¥ ì í•©í•˜ë‹¤.</p>

<p>\(k\)ì˜ ê°’ì´ ê°ì†Œí•œë‹¤ë©´ maskingì„ ëœ ìˆ˜í–‰í•˜ê²Œ ë˜ë¯€ë¡œ Encoder Inputì— ë³€í˜•ì´ ëœ ë°œìƒí•œë‹¤ëŠ” ì˜ë¯¸ì´ë©°, ë™ì‹œì— Decoder Inputìœ¼ë¡œ ë“¤ì–´ì˜¤ëŠ” ê°’ì´ ê°ì†Œí•¨ì„ ëœ»í•œë‹¤. ë”°ë¼ì„œ Encoderì— ëŒ€í•œ ì˜ì¡´ë„ë¥¼ ë†’ì´ê²Œ ëœë‹¤.</p>

<p>ë°˜ëŒ€ë¡œ \(k\)ì˜ ê°’ì´ ì¦ê°€í•œë‹¤ë©´ maskingì„ ë” ë§ì´ ìˆ˜í–‰í•˜ê²Œ ë˜ë¯€ë¡œ Encoder Inputì— ë³€í˜•ì´ ë” ë°œìƒí•œë‹¤ëŠ” ì˜ë¯¸ì´ë©°, ë™ì‹œì— Decoder Inputìœ¼ë¡œ ë“¤ì–´ì˜¤ëŠ” ê°’ì´ ì¦ê°€í•¨ì„ ëœ»í•œë‹¤. ë”°ë¼ì„œ Decoderì— ëŒ€í•œ ì˜ì¡´ë„ë¥¼ ë†’ì´ê²Œ ëœë‹¤.</p>

<p>Language Generation taskì—ì„œëŠ” Encoder(source)ì™€ Decoder(target) ì¤‘ ì–´ëŠ ìª½ìœ¼ë¡œë„ í¸í–¥ë˜ì§€ ì•Šì•„ì•¼ ì¢‹ì€ ì„±ëŠ¥ì„ ë‚˜íƒ€ë‚¼ ê²ƒì´ë‹¤. ë”°ë¼ì„œ \(k\)ê°€ \(m\)ì˜ 50%ë¼ëŠ” ìˆ˜ì¹˜ê°€ ê°€ì¥ ì í•©í•¨ì„ ì§ê´€ì ìœ¼ë¡œ ì´í•´í•  ìˆ˜ ìˆë‹¤.</p>

<p>ìœ„ì˜ Figureì—ì„œë„ ë³¼ ìˆ˜ ìˆë“¯ì´ ë‹¹ì—°í•˜ê²Œë„ \(k=1\)ì¸ ê²½ìš°(BERTì˜ MLM), \(k=m\)ì¸ ê²½ìš°(General Language Model) ëª¨ë‘ Language Generation taskì—ì„œëŠ” ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì´ì§€ ëª»í•œë‹¤.</p>

<h3 id="ablation-study-of-mass">Ablation Study of MASS</h3>

<p><img src="/assets/images/2021-01-19-MASS-Masked-Sequence-to-Sequence-Pre-training-for-Language-Generation/09-04-2020-17.37.03.jpg" alt="MASS%20Masked%20Sequence%20to%20Sequence%20Pre-training%20for%20%2071c3529aece54ebab2fa54feb4adda92/09-04-2020-17.37.03.jpg" /></p>

<p>MASSì—ì„œ ì¶”ê°€ëœ ìƒˆë¡œìš´ Masking Rule ë‹¤ìŒì˜ 2ê°€ì§€ë¡œ ì •ë¦¬í•  ìˆ˜ ìˆë‹¤.</p>

<ol>
  <li>MASK tokenì„ ì—°ì†ì ìœ¼ë¡œ ë°°ì¹˜</li>
  <li>encoder inputì˜ unmasked tokenì„ decoder inputì—ì„œ masking</li>
</ol>

<p>ìœ„ì˜ Tableì˜ DiscreteëŠ” 1ë²ˆ ruleì„ ì œê±°í•œ ê²ƒì´ê³ (ë¹„ì—°ì†ì ìœ¼ë¡œ MASK token ë°°ì¹˜), FeedëŠ” 2ë²ˆ ruleì„ ì œê±°í•œ ê²ƒì´ë‹¤(decoder inputì´ original sentence). Unsupervised English to French NMTì—ì„œ MASSê°€ ê°€ì¥ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì˜€ë‹¤.</p>

<h1 id="conclusion">Conclusion</h1>

<p>MASSëŠ” Datasetì´ ì ì€ ê²½ìš°(ë˜ëŠ” Datasetì´ ì•„ì˜ˆ ì—†ëŠ” ê²½ìš°)ì˜ Language Generation taskì—ì„œ ê¸°ì¡´ì˜ SOTAë¥¼ ëŠ¥ê°€í•˜ëŠ” ì„±ëŠ¥ì„ ë³´ì˜€ë‹¤. íŠ¹íˆë‚˜ Unsupervised NMTì—ì„œ ë¹„ì•½ì ì¸ ì„±ëŠ¥ í–¥ìƒì„ ì´ë¤„ëƒˆë‹¤.</p>
:ET