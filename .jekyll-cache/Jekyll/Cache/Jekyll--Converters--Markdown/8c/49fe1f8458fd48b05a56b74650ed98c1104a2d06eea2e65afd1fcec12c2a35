I"–T<p>Archive Link: https://arxiv.org/abs/1508.07909
Created: Sep 21, 2020 3:12 PM
Field: NLP
Paper Link: https://arxiv.org/pdf/1508.07909.pdf
Status: completed
Submit Date: Aug 15, 2015</p>

<h1 id="backgrounds">Backgrounds</h1>

<h2 id="bleu-score-bilingual-evaluation-understudy-score">BLEU Score (Bilingual Evaluation Understudy) score</h2>

\[BLEU=min\left(1,\frac{\text{output length}}{\text{reference length}}\right)\left(\prod_{i=1}^4precision_i\right)^{\frac{1}{4}}\]

<p>reference sentenceì™€ output sentenceì˜ ì¼ì¹˜ìœ¨ì„ ë‚˜íƒ€ë‚´ëŠ” scoreì´ë‹¤. 3ë‹¨ê³„ ì ˆì°¨ë¥¼ ê±°ì³ ìµœì¢… BLEU Scoreë¥¼ ë„ì¶œí•´ë‚¸ë‹¤.</p>

<ol>
  <li>n-gramì—ì„œ ìˆœì„œìŒì˜ ê²¹ì¹˜ëŠ” ì •ë„ (Precision)
    <ul>
      <li>Example
        <ul>
          <li>
            <p>output sentence</p>

            <p><strong>ë¹›ì´ ì¬ëŠ”</strong> ë…¸ì¸ì€ <strong>ì™„ë²½í•œ</strong> ì–´ë‘ìš´ ê³³ì—ì„œ <strong>ì ë“  ì‚¬ëŒê³¼ ë¹„êµí•  ë•Œ</strong> ê°•ë°•ì¦ì´ <strong>ì‹¬í•´ì§ˆ</strong> ê¸°íšŒê°€ <strong>í›¨ì”¬ ë†’ì•˜ë‹¤</strong></p>
          </li>
          <li>
            <p>true sentence</p>

            <p><strong>ë¹›ì´ ì¬ëŠ”</strong> ì‚¬ëŒì€ <strong>ì™„ë²½í•œ</strong> ì–´ë‘ ì—ì„œ <strong>ì ë“  ì‚¬ëŒê³¼ ë¹„êµí•  ë•Œ</strong> ìš°ìš¸ì¦ì´ <strong>ì‹¬í•´ì§ˆ</strong> ê°€ëŠ¥ì„±ì´ <strong>í›¨ì”¬ ë†’ì•˜ë‹¤</strong></p>
          </li>
        </ul>
      </li>
      <li>
        <p>1-gram precision</p>

\[\frac{\text{\# of correct 1-gram in output sentence}}{\text{all 1-gram pair in output sentence}}=\frac{10}{14}\]
      </li>
      <li>
        <p>2-gram precision</p>

\[\frac{\text{\# of correct 2-gram in output sentence}}{\text{all 2-gram pair in output sentence}}=\frac{5}{13}\]
      </li>
      <li>
        <p>3-gram precision</p>

\[\frac{\text{\# of correct 3-gram in output sentence}}{\text{all 3-gram pair in output sentence}}=\frac{2}{12}\]
      </li>
      <li>
        <p>4-gram precision</p>

\[\frac{\text{\# of correct 4-gram in output sentence}}{\text{all 4-gram pair in output sentence}}=\frac{1}{11}\]
      </li>
    </ul>
  </li>
  <li>ê°™ì€ ë‹¨ì–´ì— ëŒ€í•œ ë³´ì • (Clipping)
    <ul>
      <li>Example
        <ul>
          <li>
            <p>output sentence</p>

            <p><strong>The more</strong> decomposition <strong>the more</strong> flavor <strong>the</strong> food has</p>
          </li>
          <li>
            <p>true sentence</p>

            <p><strong>The more the</strong> merrier I always say</p>
          </li>
        </ul>
      </li>
      <li>
        <p>1-gram precision</p>

\[\frac{\text{\# of 1-gram in output sentence}}{\text{all 1-gram pair in output sentence}}=\frac{5}{9}\]
      </li>
      <li>
        <p>Clipping 1-gram precision</p>

\[\frac{\text{\# of 1-gram in output sentence}}{\text{all 1-gram pair in output sentence}}=\frac{3}{9}\]
      </li>
    </ul>
  </li>
  <li>ë¬¸ì¥ ê¸¸ì´ì— ëŒ€í•œ ë³´ì • (Brevity Penalty)
    <ul>
      <li>Example
        <ul>
          <li>
            <p>output sentence</p>

            <p><strong>ë¹›ì´ ì¬ëŠ”</strong> ë…¸ì¸ì€ <strong>ì™„ë²½í•œ</strong> ì–´ë‘ìš´ ê³³ì—ì„œ ì ë“¬</p>
          </li>
          <li>
            <p>true sentence</p>

            <p><strong>ë¹›ì´ ì¬ëŠ”</strong> ì‚¬ëŒì€ <strong>ì™„ë²½í•œ</strong> ì–´ë‘ ì—ì„œ ì ë“  ì‚¬ëŒê³¼ ë¹„êµí•  ë•Œ ìš°ìš¸ì¦ì´ ì‹¬í•´ì§ˆ ê°€ëŠ¥ì„±ì´ í›¨ì”¬ ë†’ì•˜ë‹¤</p>
          </li>
        </ul>
      </li>
      <li>
        <p>brevity penalty</p>

\[min\left(1,\frac{\text{\# of words in output sentence}}{\text{\# of words in true sentence}}\right)=min\left(1,\frac{6}{14}\right)=\frac{3}{7}\]
      </li>
    </ul>
  </li>
  <li>ìµœì¢… BLEU Score
    <ul>
      <li>Example
        <ul>
          <li>
            <p>output sentence</p>

            <p><strong>ë¹›ì´ ì¬ëŠ”</strong> ë…¸ì¸ì€ ì™„ë²½í•œ ì–´ë‘ìš´ ê³³ì—ì„œ <strong>ì ë“  ì‚¬ëŒê³¼ ë¹„êµí•  ë•Œ</strong> ê°•ë°•ì¦ì´ <strong>ì‹¬í•´ì§ˆ</strong> ê¸°íšŒê°€ <strong>í›¨ì”¬ ë†’ì•˜ë‹¤</strong></p>
          </li>
          <li>
            <p>true sentence</p>

            <p><strong>ë¹›ì´ ì¬ëŠ”</strong> ì‚¬ëŒì€ <strong>ì™„ë²½í•œ</strong> ì–´ë‘ ì—ì„œ <strong>ì ë“  ì‚¬ëŒê³¼ ë¹„êµí•  ë•Œ</strong> ìš°ìš¸ì¦ì´ <strong>ì‹¬í•´ì§ˆ</strong> ê°€ëŠ¥ì„±ì´ <strong>í›¨ì”¬ ë†’ì•˜ë‹¤</strong></p>
          </li>
        </ul>
      </li>
      <li>
        <p>BLEU Score</p>

\[BLEU=min\left(1,\frac{\text{output length}}{\text{reference length}}\right)\left(\prod_{i=1}^4precision_i\right)^{\frac{1}{4}}\\=min\left(1,\frac{14}{14}\right)\times\left(\frac{10}{14}\times\frac{5}{13}\times\frac{2}{12}\times\frac{1}{11}\right)^{\frac{1}{4}}\]
      </li>
    </ul>
  </li>
</ol>

<p>ì¶œì²˜: <a href="https://donghwa-kim.github.io/BLEU.html">https://donghwa-kim.github.io/BLEU.html</a></p>

<h1 id="abstract">Abstract</h1>

<p>ê¸°ì¡´ì˜ NMT (Neural machine translation)ëŠ” ëª¨ë‘ ê³ ì •ëœ ê°œìˆ˜ì˜ vocabulary ì•ˆì—ì„œ ì‘ì—…í–ˆë‹¤. í•˜ì§€ë§Œ translationì€ vocabulary ê°œìˆ˜ì˜ ì œí•œì´ ì—†ëŠ” open-vocabulary problemì´ê¸° OOV(out of vocabulary) wordê°€ ë§ì´ ë°œìƒí•  ìˆ˜ë°–ì— ì—†ë‹¤. ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ì´ëŸ¬í•œ OOV ë¬¸ì œë¥¼ subword unit í™œìš©í•´ í•´ê²°í•˜ê³ ì í–ˆë‹¤.</p>

<h1 id="introduction">Introduction</h1>

<p>ê¸°ì¡´ì˜ NMT Modelì€ OOV wordsì— ëŒ€í•´ back-off model ì‚¬ìš©í•´ì™”ë‹¤. back-off model ëŒ€ì‹  ë³¸ ë…¼ë¬¸ì—ì„œ ì œì‹œí•  subword unitì„ ì‚¬ìš©í•  ê²½ìš° OOV ë¬¸ì œë¥¼ ë” í™•ì‹¤íˆ í•´ê²°í•´ open-vocabulary problemì—ì„œ ì„±ëŠ¥ í–¥ìƒì„ ì´ëŒì–´ë‚¼ ìˆ˜ ìˆë‹¤.</p>

<h1 id="subword-translation">Subword Translation</h1>

<p>í˜„ì¬ì˜ language modelì—ì„œ translatableí•˜ì§€ ì•Šë”ë¼ë„, ë‹¤ë¥¸ languageì˜ translationì˜ sub wordë¥¼ ì‚¬ìš©í•˜ë©´ translateì´ ê°€ëŠ¥í•˜ë‹¤.</p>

<ol>
  <li>ì´ë¦„ ë“±ì˜ ê³ ìœ  ëª…ì‚¬ëŠ” ìŒì ˆ ë³„ë¡œ ëŒ€ì‘ì‹œí‚¨ë‹¤.
    <ul>
      <li>Barack Obama (English; German)</li>
      <li>Ğ‘Ğ°Ñ€Ğ°Ğº ĞĞ±Ğ°Ğ¼Ğ° (Russian)</li>
      <li>ãƒãƒ©ã‚¯ãƒ»ã‚ªãƒãƒ (ba-ra-ku o-ba-ma) (Japanese)</li>
    </ul>
  </li>
  <li>ë™ì˜ì–´, ì™¸ë˜ì–´ ë“± ê°™ì€ originì„ ê°–ëŠ” ë‹¨ì–´ë“¤ì€ ì¼ì •í•œ ê·œì¹™ì„ ê°–ê³  ë³€í˜•ë˜ë¯€ë¡œ, character-level translation ì‚¬ìš©í•œë‹¤.
    <ul>
      <li>claustrophobia (English)</li>
      <li>Klaustrophobie (German)</li>
      <li>ĞšĞ»Ğ°ÑƒÑÑ‚Ñ€Ğ¾Ñ„Ğ¾Ğ±Ğ¸Ñ (KlaustrofobiÃ¢) (Russian)</li>
    </ul>
  </li>
  <li>ë³µí•©ì–´ëŠ” ê°ê°ì˜ sub-wordë¥¼ ë²ˆì—­í•œ í›„ ê²°í•©í•œë‹¤.
    <ul>
      <li>solar system (English)</li>
      <li>Sonnensystem (Sonne + System) (German)</li>
      <li>Naprendszer (Nap + Rendszer) (Hungarian)</li>
    </ul>
  </li>
</ol>

<p>ìœ„ì™€ ê°™ì€ ê·œì¹™ìœ¼ë¡œ german training dataì—ì„œ ê°€ì¥ ë¹ˆë„ ë‚®ì€ 100ê°œì˜ wordë¥¼ ë¶„ì„í•˜ë©´ english dataë¥¼ í†µí•´ 56ê°œì˜ ë³µí•©ì–´, 21ê°œì˜ ê³ ìœ ëª…ì‚¬, 6ê°œì˜ ì™¸ë˜ì–´ ë“±ì„ ì°¾ì•„ë‚¼ ìˆ˜ ìˆì—ˆë‹¤.</p>

<h2 id="related-work">Related Work</h2>

<p>OOVëŠ” ê³ ìœ ëª…ì‚¬ (ì‚¬ëŒ ì´ë¦„, ì§€ì—­ëª…), ì™¸ë˜ì–´ ë“±ì— ëŒ€í•´ì„œ ìì£¼ ë°œìƒí•œë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ character levelë¡œ wordë¥¼ ë¶„ë¦¬í•œ ë’¤, ê° characterë“¤ì´ ì¼ì •í•œ ê¸°ì¤€ì„ ì¶©ì¡±í•  ê²½ìš° í•˜ë‚˜ì˜ tokenìœ¼ë¡œ ë¬¶ì–´ í‘œí˜„í•˜ëŠ” ë°©ì‹ì„ ì±„íƒí–ˆë‹¤. ì´ë¥¼ í†µí•´ text sizeëŠ” ì¤„ì–´ë“¤ê²Œ ëœë‹¤. ì´ ë•Œ ë‹¨ì–´ë¥¼ subwordë¡œ êµ¬ë¶„í•˜ëŠ” ê¸°ì¡´ì˜ Segmentation algorithmì„ ì‚¬ìš©í•˜ë˜,  ì¢€ ë” aggressiveí•œ ê¸°ì¤€ì„ ì ìš©í•˜ê³ ì í–ˆë‹¤. vocabulary sizeì™€ text sizeëŠ” ì„œë¡œ trade-off ê´€ê³„ì´ë¯€ë¡œ vocabulary sizeê°€ ê°ì†Œí•œë‹¤ë©´ ì‹œê°„/ê³µê°„ ë³µì¡ë„ëŠ” ë‚®ì•„ì§€ê² ì§€ë§Œ  unknown wordì˜ ê°œìˆ˜ê°€ ì¦ê°€í•˜ê²Œ ëœë‹¤.</p>

<h2 id="byte-pair-encoding-bpe">Byte Pair Encoding (BPE)</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">re</span><span class="p">,</span> <span class="n">collections</span>

<span class="k">def</span> <span class="nf">get_stats</span><span class="p">(</span><span class="n">vocab</span><span class="p">):</span>
	<span class="n">pairs</span> <span class="o">=</span> <span class="n">collections</span><span class="p">.</span><span class="n">defaultdict</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
	<span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">freq</span> <span class="ow">in</span> <span class="n">vocab</span><span class="p">.</span><span class="n">items</span><span class="p">():</span>
		<span class="n">symbols</span> <span class="o">=</span> <span class="n">word</span><span class="p">.</span><span class="n">split</span><span class="p">()</span>
		<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">symbols</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span>
			<span class="n">pairs</span><span class="p">[</span><span class="n">symbols</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">symbols</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]]</span> <span class="o">+=</span> <span class="n">freq</span>
	<span class="k">return</span> <span class="n">pairs</span>

<span class="k">def</span> <span class="nf">merge_vocab</span><span class="p">(</span><span class="n">pair</span><span class="p">,</span> <span class="n">v_in</span><span class="p">):</span>
	<span class="n">v_out</span> <span class="o">=</span> <span class="p">{}</span>
	<span class="n">bigram</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="n">escape</span><span class="p">(</span><span class="s">' '</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">pair</span><span class="p">))</span>
	<span class="n">p</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="nb">compile</span><span class="p">(</span><span class="sa">r</span><span class="s">'(?&lt;!\S)'</span> <span class="o">+</span> <span class="n">bigram</span> <span class="o">+</span> <span class="sa">r</span><span class="s">'(?!\S)'</span><span class="p">)</span>
	<span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">v_in</span><span class="p">:</span>
		<span class="n">w_out</span> <span class="o">=</span> <span class="n">p</span><span class="p">.</span><span class="n">sub</span><span class="p">(</span><span class="s">''</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">pair</span><span class="p">),</span> <span class="n">word</span><span class="p">)</span>
		<span class="n">v_out</span><span class="p">[</span><span class="n">w_out</span><span class="p">]</span> <span class="o">=</span> <span class="n">v_in</span><span class="p">[</span><span class="n">word</span><span class="p">]</span>
	<span class="k">return</span> <span class="n">v_out</span>

<span class="n">vocab</span> <span class="o">=</span> <span class="p">{</span><span class="s">'low&lt;/w&gt;'</span> <span class="p">:</span> <span class="mi">5</span><span class="p">,</span> <span class="s">'lower&lt;/w&gt;'</span> <span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
				 <span class="s">'newest&lt;/w&gt;'</span> <span class="p">:</span> <span class="mi">6</span><span class="p">,</span> <span class="s">'widest&lt;/w&gt;'</span> <span class="p">:</span> <span class="mi">3</span><span class="p">}</span>

<span class="n">num_merges</span> <span class="o">=</span> <span class="mi">10</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_merges</span><span class="p">):</span>
	<span class="n">pairs</span> <span class="o">=</span> <span class="n">get_stats</span><span class="p">(</span><span class="n">vocab</span><span class="p">)</span>
	<span class="n">best</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">pairs</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="n">pairs</span><span class="p">.</span><span class="n">get</span><span class="p">)</span>
	<span class="n">vocab</span> <span class="o">=</span> <span class="n">merge_vocab</span><span class="p">(</span><span class="n">best</span><span class="p">,</span> <span class="n">vocab</span><span class="p">)</span>
	<span class="k">print</span><span class="p">(</span><span class="n">best</span><span class="p">)</span>

<span class="c1"># r .  -&gt;  r.
# l o  -&gt;  lo
# lo w -&gt;  low
# e r. -&gt;  er.
</span></code></pre></div></div>

<p>BPEëŠ” ê°€ì¥ ë¹ˆë„ê°€ ë†’ì€ pair of bytesë¶€í„° í•˜ë‚˜ì˜ single byteë¡œ ì¹˜í™˜í•´ ì €ì¥í•˜ëŠ” ì••ì¶• algorithmì´ë‹¤.</p>

<p>BPEëŠ” ë‹¤ìŒê³¼ ê°™ì€ ê³¼ì •ì„ ë”°ë¥¸ë‹¤.</p>

<ol>
  <li>wordë¥¼ characterì˜ sequenceë¡œ ë³€í™˜ í›„ end symbol  Â·  ì¶”ê°€</li>
  <li>ëª¨ë“  characterì˜ pairë¥¼ ì„¼ í›„ ê°€ì¥ ë¹ˆë„ê°€ ë†’ì€ pair of character (â€˜Aâ€™, â€˜Bâ€™)ë¥¼ ìƒˆë¡œìš´ symbol â€˜ABâ€™ (character n-gram)ë¡œ ì¹˜í™˜</li>
  <li>2ë²ˆ ë‹¨ê³„ë¥¼ ì›í•˜ëŠ” íšŸìˆ˜ë§Œí¼(vocabulary sizeë§Œí¼ tokenì´ ìƒì„±ë  ë•Œê¹Œì§€) ë°˜ë³µ</li>
</ol>

<p>BPEì˜ ë°˜ë³µ íšŸìˆ˜ëŠ” vocabulary sizeë¼ëŠ” hyperparameterì— ë”°ë¼ ê²°ì •ëœë‹¤.</p>

<ul>
  <li>ì˜ˆì‹œ
    <ol>
      <li>
        <p>train sentences</p>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="n">sentence</span> <span class="o">=</span> <span class="p">[</span> 
 <span class="s">'black bug bit a black bear but is the black bear that the big black bug bit'</span><span class="p">,</span>
 <span class="s">'a big bug bit the little beetle but the little beetle bit the big bug back'</span><span class="p">.</span>
 <span class="s">'the better with the butter is the batter that is better'</span>
 <span class="p">]</span>
</code></pre></div>        </div>
      </li>
      <li>
        <p>count segments</p>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="p">[(</span><span class="s">'t h e &lt;/w&gt;'</span><span class="p">,</span> <span class="mi">8</span><span class="p">),</span> <span class="p">(</span><span class="s">'b l a c k &lt;/w&gt;'</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="p">(</span><span class="s">'b i t &lt;/w&gt;'</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span>
  <span class="p">(</span><span class="s">'i s &lt;/w&gt;'</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="p">(</span><span class="s">'b i g &lt;/w&gt;'</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="p">(</span><span class="s">'b e a r &lt;/w&gt;'</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
  <span class="p">(</span><span class="s">'b u t &lt;/w&gt;'</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="p">(</span><span class="s">'t h a t &lt;/w&gt;'</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="p">(</span><span class="s">'l i t t l e &lt;/w&gt;'</span><span class="p">,</span> <span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
  <span class="p">(</span><span class="s">'b e e t l e &lt;/w&gt;'</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="p">(</span><span class="s">'b e t t e r &lt;/w&gt;'</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="p">(</span><span class="s">'b a c k &lt;/w&gt;'</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
  <span class="p">(</span><span class="s">'w i t h &lt;/w&gt;'</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="s">'b u t t e r &lt;/w&gt;'</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="s">'b a t t e r &lt;/w&gt;'</span><span class="p">,</span> <span class="mi">1</span><span class="p">)]</span>
</code></pre></div>        </div>
      </li>
      <li>
        <p>count bi-grams</p>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="p">[((</span><span class="s">'t'</span><span class="p">,</span> <span class="s">'h'</span><span class="p">),</span> <span class="mi">11</span><span class="p">),</span>
  <span class="p">((</span><span class="s">'h'</span><span class="p">,</span> <span class="s">'e'</span><span class="p">),</span> <span class="mi">8</span><span class="p">).</span>
  <span class="p">((</span><span class="s">'t'</span><span class="p">,</span> <span class="s">'&lt;/w&gt;'</span><span class="p">),</span> <span class="mi">8</span><span class="p">),</span>
  <span class="p">((</span><span class="s">'g'</span><span class="p">,</span> <span class="s">'&lt;/w&gt;'</span><span class="p">),</span> <span class="mi">7</span><span class="p">)]</span>
</code></pre></div>        </div>
      </li>
      <li>
        <p>add merge-rules</p>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="p">(</span><span class="s">'t'</span><span class="p">,</span> <span class="s">'h'</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">th</span>
 <span class="p">(</span><span class="s">'h'</span><span class="p">,</span> <span class="s">'e'</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">he</span>
 <span class="p">(</span><span class="s">'t'</span><span class="p">,</span> <span class="s">'&lt;/w&gt;'</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">t</span><span class="o">&lt;/</span><span class="n">w</span><span class="o">&gt;</span>
 <span class="p">(</span><span class="s">'g'</span><span class="p">,</span> <span class="s">'&lt;/w&gt;'</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">g</span><span class="o">&lt;/</span><span class="n">w</span><span class="o">&gt;</span>
</code></pre></div>        </div>
      </li>
    </ol>
  </li>
</ul>

<h1 id="evaluation">Evaluation</h1>

<h2 id="subword-statistics">Subword statistics</h2>

<p><img src="/assets/images/2021-01-19-Neural-Machine-Translation-of-Rare-Words-with-Subword-Units/05-09-2020-21.10.22.jpg" alt="Neural%20Machine%20Translation%20of%20Rare%20Words%20with%20Subw%203301351401254a21af391ffcd056405b/05-09-2020-21.10.22.jpg" /></p>

<ul>
  <li>
    <h1 id="tokens-text-size">tokens: text size</h1>
  </li>
  <li>
    <h1 id="types-vocabulary-size-token-ê°œìˆ˜">types: vocabulary size, token ê°œìˆ˜</h1>
  </li>
  <li>
    <h1 id="unk-unknown-word-oov-wordì˜-ê°œìˆ˜">UNK: unknown word (OOV word)ì˜ ê°œìˆ˜</h1>
  </li>
</ul>

<h2 id="translation-experiments">Translation experiments</h2>

<p><img src="/assets/images/2021-01-19-Neural-Machine-Translation-of-Rare-Words-with-Subword-Units/05-09-2020-21.22.15.jpg" alt="Neural%20Machine%20Translation%20of%20Rare%20Words%20with%20Subw%203301351401254a21af391ffcd056405b/05-09-2020-21.22.15.jpg" /></p>

<p><img src="/assets/images/2021-01-19-Neural-Machine-Translation-of-Rare-Words-with-Subword-Units/05-09-2020-21.24.02.jpg" alt="Neural%20Machine%20Translation%20of%20Rare%20Words%20with%20Subw%203301351401254a21af391ffcd056405b/05-09-2020-21.24.02.jpg" /></p>

<ul>
  <li>W Unk: back-off dictionaryë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šì€ modelì´ë‹¤.</li>
  <li>W Dict: back-off dictionaryë¥¼ ì‚¬ìš©í•œ modelì´ë‹¤.</li>
  <li>C2-50k: char-bigramì„ ì‚¬ìš©í•œ modelì´ë‹¤.</li>
  <li>CHR F3: ì¸ê°„ì˜ íŒë‹¨ê³¼ ì¼ì¹˜ìœ¨</li>
  <li>unigram F1: BLEU unigram(brevity penalty ì œì™¸)ì™€ Recallì˜ ì¡°í•©</li>
</ul>

<p>sourceì™€ target ê°ê° ë”°ë¡œ BPEë¥¼ ìˆ˜í–‰í•˜ëŠ” BPEë³´ë‹¤ ë™ì‹œì— ìˆ˜í–‰í•˜ëŠ” BPE jointê°€ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì˜€ë‹¤.</p>

<h1 id="analysis">Analysis</h1>

<h2 id="unigram-accuracy">Unigram accuracy</h2>

<p><img src="/assets/images/2021-01-19-Neural-Machine-Translation-of-Rare-Words-with-Subword-Units/05-09-2020-22.04.35.jpg" alt="Neural%20Machine%20Translation%20of%20Rare%20Words%20with%20Subw%203301351401254a21af391ffcd056405b/05-09-2020-22.04.35.jpg" /></p>

<p><img src="/assets/images/2021-01-19-Neural-Machine-Translation-of-Rare-Words-with-Subword-Units/05-09-2020-23.20.17.jpg" alt="Neural%20Machine%20Translation%20of%20Rare%20Words%20with%20Subw%203301351401254a21af391ffcd056405b/05-09-2020-23.20.17.jpg" /></p>

<h2 id="manual-analysis">Manual Analysis</h2>

<p><img src="/assets/images/2021-01-19-Neural-Machine-Translation-of-Rare-Words-with-Subword-Units/12-01-2020-01.37.29.jpg" alt="Neural%20Machine%20Translation%20of%20Rare%20Words%20with%20Subw%203301351401254a21af391ffcd056405b/12-01-2020-01.37.29.jpg" /></p>

<p><img src="/assets/images/2021-01-19-Neural-Machine-Translation-of-Rare-Words-with-Subword-Units/12-01-2020-01.37.33.jpg" alt="Neural%20Machine%20Translation%20of%20Rare%20Words%20with%20Subw%203301351401254a21af391ffcd056405b/12-01-2020-01.37.33.jpg" /></p>

<h1 id="conclusion">Conclusion</h1>

<p>OOV ë¬¸ì œë¥¼ í•´ê²°í•´ NMTì™€ ê°™ì€ open-vocabulary translationì—ì„œ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì˜€ë‹¤. ê¸°ì¡´ì— OOVë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì‚¬ìš©ë˜ë˜ back-off translation modelë³´ë‹¤ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì˜€ë‹¤.</p>
:ET