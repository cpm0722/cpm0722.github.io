I"¦?<h1 id="xlnet-generalized-autoregressive-pretraining-for-language-understanding">XLNet: Generalized Autoregressive Pretraining for Language Understanding</h1>
<p>title: XLNet: Generalized Autoregressive Pretraining for Language Understanding
subtitle: XLNet
categories: Paper Review
tags: NLP
date: 2021-01-19 12:59:48 +0000
last_modified_at: 2021-01-19 12:59:48 +0000
â€”</p>

<p>Archive Link: https://arxiv.org/abs/1906.08237
Created: Sep 21, 2020 3:18 PM
Field: NLP
Paper Link: https://arxiv.org/pdf/1906.08237.pdf
Status: not checked
Submit Date: Jun 19, 2019</p>

<h1 id="introduction">Introduction</h1>

<p>Unsupervised Learningì„ pretrainingì— ì ìš©ì‹œí‚¤ëŠ” ë°©ì‹ì€ NLP domainì—ì„œ ë§¤ìš° í° ì„±ê³¼ë¥¼ ì´ë¤„ëƒˆë‹¤. Unsupervised pretrainingí•˜ëŠ” ë°©ë²•ë¡ ì€ í¬ê²Œ AutoRegressive(AR)ê³¼ AutoEncoding(AE)ê°€ ìˆë‹¤. AutoRegressiveëŠ” ìˆœë°©í–¥ ë˜ëŠ” ì—­ë°©í–¥ìœ¼ë¡œ ë‹¤ìŒì˜ corpusë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ í•™ìŠµí•œë‹¤. ì´ëŠ” ë‹¨ë°©í–¥ contextë§Œ í•™ìŠµí•  ìˆ˜ ìˆë‹¤ëŠ” ë‹¨ì ì´ ìˆë‹¤. í•˜ì§€ë§Œ í˜„ì‹¤ì˜ ëŒ€ë¶€ë¶„ì˜ downstream taskëŠ” bidirectional contextê°€ í•„ìˆ˜ì ì´ê¸°ì— ì´ëŠ” í¬ë‚˜í° í•œê³„ê°€ ëœë‹¤.</p>

<p>ë°˜ë©´ AutoEncodingì€ ë³€í˜•ëœ inputì„ ë‹¤ì‹œ ë³¸ë˜ì˜ inputìœ¼ë¡œ ì¬êµ¬ì„±í•˜ëŠ” ë°©ì‹ì´ë‹¤. BERTê°€ ëŒ€í‘œì ì¸ ì˜ˆì‹œì¸ë°, input dataì˜ ì¼ë¶€ë¶„ì„ [MASK] token ë“±ìœ¼ë¡œ ë³€í™”ë¥¼ ì¤€ ë’¤, ì›ë˜ì˜ inputì„ ë§Œë“¤ì–´ë‚´ë„ë¡ í•™ìŠµì‹œí‚¨ë‹¤. ì´ëŸ¬í•œ ë°©ë²•ì€ bidirectional contextë¥¼ í•™ìŠµí•  ìˆ˜ ìˆë‹¤ëŠ” ì ì—ì„œ AutoRegressiveì— ë¹„í•´ ìƒëŒ€ì ìœ¼ë¡œ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì¸ë‹¤.í•˜ì§€ë§Œ ì¸ìœ„ì ì¸ ë³€í˜•ì„ ê°€í•´ ë§Œë“¤ì–´ë‚¸ [MASK] token ë“±ì€ pretraining ê³¼ì •ì—ì„œë§Œ ì¡´ì¬í•˜ëŠ” tokenì´ê³ , ì´í›„ downstream taskë¥¼ í•™ìŠµì‹œí‚¤ëŠ” fine-tuning ê³¼ì •ì—ì„œëŠ” ì¡´ì¬í•˜ì§€ ì•ŠëŠ” tokenì´ ëœë‹¤. ë”°ë¼ì„œ pre-trainingê³¼ fine-tuning ì‚¬ì´ì˜ ê´´ë¦¬ê°€ ë°œìƒí•˜ê²Œ ëœë‹¤. ë˜í•œ ê° [MASK] tokenì„ predictí•˜ëŠ” ê³¼ì •ì€ independentí•˜ê¸° ë•Œë¬¸ì— predictëœ tokenë“¤ ì‚¬ì´ì˜ dependencyëŠ” í•™ìŠµí•  ìˆ˜ ì—†ë‹¤ëŠ” í•œê³„ë„ ìˆë‹¤.</p>

<p>ë³¸ ë…¼ë¬¸ì—ì„œ ì œì‹œí•˜ëŠ” XLNetì€ ì´ëŸ¬í•œ ARê³¼ AEì„ ëª¨ë‘ ì‚¬ìš©í•´ ê°ê°ì˜ ì¥ì ë§Œì„ ì·¨í•˜ë„ë¡ í–ˆë‹¤.</p>

<h1 id="proposed-method">Proposed Method</h1>

<h2 id="background">Background</h2>

<h3 id="ar-autoregressive">AR (Autoregressive)</h3>

<p>ì¼ë°˜ì ì¸ ARì˜ objective functionì€ ë‹¤ìŒê³¼ ê°™ë‹¤.</p>

\[\underset{\theta}{max}\ \ log{\ p_\theta(x)} = \sum_{t=1}^Tlog{\ p_\theta\left(x_t|x_{&lt;t}\right)} = \sum_{t=1}^Tlog{\ \frac{exp\left(h_\theta\left(x_{1:t-1}\right)^Te\left(x_t\right)\right)}{\sum_{x'}{exp\left(h_\theta\left(x_{1:t-1}\right)^Te\left(x'\right)\right)}}}\]

<p>\(h_\theta\left(x_{1:t-1}\right)\)ëŠ” modelì˜ context representationì´ê³ , \(e\left(x'\right)\)ëŠ” xì˜ embeddingì´ë‹¤.</p>

<h3 id="ae-autoencoding">AE (Autoencoding)</h3>

<p>ì¼ë°˜ì ì¸ AEì˜ objective functionì€ ë‹¤ìŒê³¼ ê°™ë‹¤.</p>

\[\underset\theta{max}\ log{\ p_\theta\left(\bar{x}|\hat{x}\right)} \approx \sum_{t=1}^T{m_tlog{\ p_\theta\left(x_t|\hat{x}\right)}} = \sum_{t=1}^T{m_tlog{\ \frac{exp\left(H_\theta\left(\hat{x}\right)_t^Te\left(x_t\right)\right)}{\sum_{x'}{exp\left(H_\theta\left(\hat{x}\right)_t^Te\left(x'\right)\right)}}}}\]

<p>\(\hat{x}\)ëŠ” [MASK] token ë“±ì´ ì¶”ê°€ëœ ë³€í˜•ëœ inputì´ê³ , \(\bar{x}\)ëŠ” masked tokenì´ë‹¤.</p>

<p>\(m_t=1\)ì¸ ê²½ìš° \(x_t\)ê°€ maskedëœ ê²½ìš°ë¥¼ ëœ»í•˜ê³ , \(H_\theta\)ëŠ” Transformerì˜ hidden vectorë¥¼ ëœ»í•œë‹¤.</p>

<h3 id="xlnet">XLNet</h3>

<p>XLNetì€ ARì™€ AEë¥¼ ì•„ë˜ì˜ 3ê°€ì§€ ê´€ì ì—ì„œ ë¹„êµí•˜ë©° ê°ê°ì˜ ì¥ì ë§Œ ì·¨í•œë‹¤.</p>

<ul>
  <li>
    <p>Independence Assumption</p>

    <p>AEì˜ objective functionì€ ì¡°ê±´ë¶€í™•ë¥ ì„ ê³„ì‚°í•˜ëŠ” ê²ƒì´ë‹¤. ì´ ë•Œ \(\approx\)ë¥¼ ì‚¬ìš©í•œë‹¤. ì´ëŠ” ëª¨ë“  \(\bar{x}\)ì— ëŒ€í•œ reconstructionì´ independentí•˜ê²Œ ì´ë£¨ì–´ì§„ë‹¤ëŠ” ê°€ì • í•˜ì— ì´ë£¨ì–´ì§€ê¸° ë•Œë¬¸ì´ë‹¤. ë°˜ë©´ ARì˜ objective functionì€ ì´ëŸ¬í•œ ê°€ì • ì—†ì´ë„ ì„±ë¦½í•˜ê¸°ì— \(=\)ë¥¼ ì‚¬ìš©í•œë‹¤.</p>
  </li>
  <li>
    <p>Input Noise</p>

    <p>AEì—ì„œëŠ” [MASK] tokenê³¼ ê°™ì´ ì‹¤ì œ inputì— ì—†ë˜ tokenë“¤ì´ ì¶”ê°€ë˜ê²Œ ëœë‹¤. ì´ëŠ” pretraining ë•Œì—ë§Œ ì¡´ì¬í•˜ëŠ” tokenìœ¼ë¡œ fine-tuning ê³¼ì •ì—ì„œëŠ” ì¡´ì¬í•˜ì§€ ì•ŠëŠ”ë‹¤. ì´ëŸ¬í•œ pretrainingê³¼ fine-tuning ì‚¬ì´ì˜ ê´´ë¦¬ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ BERTì—ì„œëŠ” maskingì— ëŒ€í•´ ëª¨ë‘ [MASK] tokenìœ¼ë¡œ ë³€ê²½í•˜ì§€ ì•Šê³  ì¼ë¶€ë¶„ì€ original token ê·¸ëŒ€ë¡œ ë‘ëŠ” ë“±ì˜ ê¸°ë²•ì„ ì‚¬ìš©í–ˆìœ¼ë‚˜, ì´ëŠ” ì „ì²´ tokenì—ì„œ ê·¹íˆ ì¼ë¶€ë¶„ì—ë§Œ ì ìš©ë˜ê¸° ë•Œë¬¸ì— (0.15 * 0.1 == 0.015) ì˜ë¯¸ìˆëŠ” ê²°ê³¼ë¥¼ ë„ì¶œí•´ë‚´ì§€ ëª»í•œë‹¤. ARì—ì„œëŠ” inputì— ëŒ€í•œ ë³€ê²½ì´ ì—†ê¸° ë•Œë¬¸ì— ì´ëŸ¬í•œ ë¬¸ì œê°€ ë°œìƒí•˜ì§€ ì•ŠëŠ”ë‹¤.</p>
  </li>
  <li>
    <p>Context Dependency</p>

    <p>AEëŠ” bidirectional contextë¥¼ ëª¨ë‘ í•™ìŠµí•  ìˆ˜ ìˆì§€ë§Œ, ARì€ unidirectional contextë§Œ í•™ìŠµí•œë‹¤.</p>
  </li>
</ul>

<h2 id="objective-permutation-language-modeling">Objective: Permutation Language Modeling</h2>

<p>ARì˜ ì¥ì ì€ ëª¨ë‘ ì·¨í•˜ë©´ì„œ(no Indepence Assumption, no Input Noise) ARì˜ ë‹¨ì ì€ í•´ê²°í•˜ëŠ”(Bidirectional Context) Objective functionì„ ì •ì˜í•˜ê¸°ë¡œ í•œë‹¤.</p>

\[\underset{\theta}{max} = E_{z\thicksim Z_T}\left[\sum_{t=1}^T{log{\ p_\theta\left(x_{z_t}|x_{z_{&lt;t}}\right)}}\right]\]

<p>\(Z_T\)ëŠ” ê¸¸ì´ê°€ \(T\)ì¸ sequenceì˜ ëª¨ë“  ìˆœì—´ ì§‘í•© ì„ ëœ»í•˜ê³ , \(z_t\)ëŠ” \(Z_T\)ì—ì„œ \(t\)ë²ˆì§¸ elementë¥¼ ëœ»í•œë‹¤. \(z_{&lt;t}\)ëŠ” \(Z_T\)ì—ì„œ \(0\) ~ \(t-1\)ë²ˆì§¸ ì›ì†Œë“¤ì„ ëœ»í•œë‹¤.</p>

<p>ìœ„ì˜ Objective Functionì€ \(x_i\)ì— ëŒ€í•´ \(x_i\)ë¥¼ ì œì™¸í•œ ëª¨ë“  \(x_t\)ë¥¼ ì „ì²´ ì§‘í•©ìœ¼ë¡œ í•˜ëŠ” ìˆœì—´ì— ëŒ€í•´ likelihoodë¥¼ êµ¬í•˜ê²Œ ëœë‹¤. ARì˜ êµ¬ì¡°ë¥¼ ì±„íƒí–ˆìœ¼ë‚˜ ìˆœì—´ì„ ì‚¬ìš©í•´ bidirectional contextê¹Œì§€ í•™ìŠµí•˜ë„ë¡ í•œ ê²ƒì´ë‹¤.</p>

<h2 id="architecture-two-stream-self-attention-for-target-aware-representations">Architecture: Two-Stream Self-Attention for Target-Aware Representations</h2>

<p>ì¼ë°˜ì ì¸ Transformerì˜ Self-Attention êµ¬ì¡°ì—ì„œëŠ” Query, Key, Valueê°€ ëª¨ë‘ ê°™ì€ ê°’ìœ¼ë¡œ ì‹œì‘í•˜ê²Œ ëœë‹¤. ì¦‰, í•˜ë‚˜ì˜ hidden stateì˜ ê°’ì„ ê³µìœ í•œë‹¤. ê·¸ëŸ¬ë‚˜ XLNetì—ì„œëŠ” êµ¬ì¡°ìƒ Queryì˜ ê°’ê³¼ Key, Valueì˜ ê°’ì´ ë¶„ë¦¬ë˜ì–´ì•¼ í•œë‹¤. ì´ë¥¼ ìœ„í•´ ìƒˆë¡œìš´ representationì„ ì¶”ê°€í•˜ê²Œ ëœë‹¤.</p>

<p>êµ¬ì²´ì ì¸ ì˜ˆì‹œë¥¼ ë“¤ì–´ë³´ì. \(T = 4\)ì¼ ë•Œ, ë‘ê°€ì§€ì˜ ìˆœì—´ì´ ì„ íƒë˜ì—ˆë‹¤ê³  í•˜ì.</p>

\[Z_1 = [x_2,x_3,x_1,x_4]\]

\[Z_2 = [x_2,x_3,x_4,x_1]\]

<p>\(Z_1\)ì—ì„œ \(t=3\)ì— ëŒ€í•œ ì¡°ê±´ë¶€ í™•ë¥ ì„ êµ¬í•˜ëŠ” ì‹ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.</p>

\[p\left(x_1|x_{z_{&lt;3}}\right) =p\left(x_1|x_2,x_3\right)=\frac{exp\left(e\left(x_1\right)^Th_\theta\left(x_2,x_3\right)\right)}{\sum_{x'}{exp\left(e\left(x'\right)^Th_\theta\left(x_2,x_3\right)\right)}}\]

<p>\(Z_2\)ì—ì„œ \(t=3\)ì— ëŒ€í•œ ì¡°ê±´ë¶€ í™•ë¥ ì„ êµ¬í•˜ëŠ” ì‹ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.</p>

\[p\left(x_4|x_{z_{&lt;3}}\right) =p\left(x_4|x_2,x_3\right)=\frac{exp\left(e\left(x_4\right)^Th_\theta\left(x_2,x_3\right)\right)}{\sum_{x'}{exp\left(e\left(x'\right)^Th_\theta\left(x_2,x_3\right)\right)}}\]

<p>ìœ„ì˜ ë‘ ì¡°ê±´ë¶€í™•ë¥  ì‹ì€ ë¶„ëª¨ëŠ” ì™„ì „íˆ ê°™ì€ ê°’ì´ë‹¤. ë§Œì•½ \(x_1\)ê³¼ \(x_4\)ê°€ ê°™ì€ wordì˜€ë‹¤ê³  í•œë‹¤ë©´ (a, an, theì™€ ê°™ì€ ê´€ì‚¬ ë“±) ì™„ì „íˆ ê°™ì€ ì¡°ê±´ë¶€ í™•ë¥ ì„ ê³„ì‚°í•˜ëŠ” ìƒí™©ì´ ë°œìƒí•˜ê²Œ ëœë‹¤. ì§ì „ ì‹œì  \(t-1\)ê¹Œì§€ì˜ ì •ë³´ embedding ì •ë³´ë§Œì„ ì €ì¥í•˜ëŠ” representation \(h_\theta\left(x_{z_{&lt;t}}\right)\)ë§Œìœ¼ë¡œëŠ” ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•  ìˆ˜ ì—†ë‹¤. ë”°ë¼ì„œ í˜„ì¬ ì‹œì ì˜ ìœ„ì¹˜ì •ë³´ ê¹Œì§€ ë°›ëŠ” ìƒˆë¡œìš´ representation \(g_\theta\left(x_{z_{&lt;t}},z_t\right)\)ì„ ì¶”ê°€í•œë‹¤.  ìµœì¢…ì ìœ¼ë¡œ ì•„ë˜ì˜ ìˆ˜ì‹ì„ ì •ì˜í•˜ê²Œ ëœë‹¤.</p>

\[p\left(X_{z_t}=x|x_{z_{&lt;t}}\right) =\frac{exp\left(e\left(x\right)^Tg_\theta\left(x_{z&lt;t},z_t\right)\right)}{\sum_{x'}{exp\left(e\left(x'\right)^Tg_\theta\left(x_{z&lt;t},z_t\right)\right)}}\]

<p>ë‘ representationì— ëŒ€í•´ ìì„¸íˆ ì•Œì•„ë³´ì.</p>

<h3 id="content-representation">Content Representation</h3>

<p>\(h_\theta\left(x_{z&lt;t}\right)\)ëŠ” ê¸°ì¡´ Transformerì˜ hidden stateì™€ ë™ì¼í•œ êµ¬ì¡°ì´ë‹¤. í˜„ì¬ ì‹œì (\(t\))ì˜ ì •ë³´ê¹Œì§€ í¬í•¨í•´ ì…ë ¥ìœ¼ë¡œ ë°›ëŠ”ë‹¤. ì´ë¥¼ Content Representationì´ë¼ê³  í•˜ê³ , Key, Valueì— ì‚¬ìš©í•˜ê²Œ ëœë‹¤.</p>

<p><img src="/assets/images/2021-01-19-XLNet-Generalized-Autoregressive-Pretraining-for-Language-Understanding/08-29-2020-18.49.21.jpg" alt="XLNet%20Generalized%20Autoregressive%20Pretraining%20for%20L%20833f510b35954da883906c9bc6b15f9d/08-29-2020-18.49.21.jpg" /></p>

<h3 id="query-representation">Query Representation</h3>

<p>\(g_\theta\left(x_{z_{&lt;t}},z_t\right)\)ëŠ” í˜„ì¬ ì‹œì (\(t\))ì˜ ì •ë³´ëŠ” ì œì™¸í•˜ê³  ì…ë ¥ìœ¼ë¡œ ë°›ëŠ”ë‹¤. ëŒ€ì‹  í˜„ì¬ ì‹œì (\(t\))ì˜ ìœ„ì¹˜ ì •ë³´(\(z_t\))ëŠ” ì…ë ¥ìœ¼ë¡œ ë°›ëŠ”ë‹¤. ì´ë¥¼ Query Representationì´ë¼ê³  í•˜ê³ , Queryì— ì‚¬ìš©í•˜ê²Œ ëœë‹¤.</p>

<p><img src="/assets/images/2021-01-19-XLNet-Generalized-Autoregressive-Pretraining-for-Language-Understanding/08-29-2020-18.49.27.jpg" alt="XLNet%20Generalized%20Autoregressive%20Pretraining%20for%20L%20833f510b35954da883906c9bc6b15f9d/08-29-2020-18.49.27.jpg" /></p>

<h3 id="permutation-language-modeling-with-two-stream-attention">Permutation Language Modeling with Two-Stream Attention</h3>

<p>ì „ì²´ì ì¸ Two-Stream Attentionì˜ êµ¬ì¡°ëŠ” ì•„ë˜ì™€ ê°™ë‹¤.</p>

<p><img src="/assets/images/2021-01-19-XLNet-Generalized-Autoregressive-Pretraining-for-Language-Understanding/08-29-2020-18.52.01.jpg" alt="XLNet%20Generalized%20Autoregressive%20Pretraining%20for%20L%20833f510b35954da883906c9bc6b15f9d/08-29-2020-18.52.01.jpg" /></p>

<p>Queryì˜ ì´ˆê¸°ê°’ì€ weight \(w\), Keyì™€ Valueì˜ ì´ˆê¸° ê°’ì€ embeddingëœ input ê°’ \(e\left(x_i\right)\)ì´ë‹¤. ì´í›„ ì•„ë˜ì™€ ê°™ì´ ê°±ì‹ ëœë‹¤.</p>

<p>Query Streamì€ í˜„ì¬ ì‹œì  \(t\)ì˜ ìœ„ì¹˜ ì •ë³´(\(z_t\))ëŠ” ì•Œ ìˆ˜ ìˆì§€ë§Œ, ì‹¤ì œ ê°’(\(x_{z_t}\))ëŠ” ì•Œì§€ ëª»í•˜ëŠ” ìƒíƒœë¡œ êµ¬í•´ì§„ë‹¤.</p>

<p>Content Streamì€ í˜„ì¬ ì‹œì  \(t\)ì˜ ìœ„ì¹˜ ì •ë³´(\(z_t\))ëŠ” ë¬¼ë¡ , ì‹¤ì œ ê°’(\(x_{z_t}\))ë„ ì‚¬ìš©í•´ êµ¬í•´ì§„ë‹¤.</p>

\[g_{z_t}^{\left(m\right)} = Attention\left(Q=g_t^{\left(m-1\right)},KV=h_{z_{&lt;t}}^{\left(m-1\right)};\theta\right)\]

\[h_{z_t}^{\left(m\right)}=Attention\left(Q=h_{z_t}^{\left(m-1\right)},KV=h_{z_{z\leq t}}^{\left(m-1\right)};\theta\right)\]

<p>\(m\)ì€ Multi-head Atention Layerì˜ í˜„ì¬ Layer Numberì´ë‹¤.</p>

<h2 id="incorporating-ideas-from-transformer-xl">Incorporating Ideas from Transformer-XL</h2>

<h2 id="modeling-multiple-segments">Modeling Multiple Segments</h2>

<p>BERTì˜ inputê³¼ ë™ì¼í•œ êµ¬ì¡°ë¥¼ ì±„íƒí–ˆë‹¤. [CLS, A, SEP, B, SEP]ì˜ êµ¬ì¡°ì´ë‹¤. [CLS], [SEP] tokenì€ BERTì™€ ë™ì¼í•œ ì—­í• ì´ê³ , [A], [B]ëŠ” ê°ê° sentence A, sentence Bì´ë‹¤. BERTì™€ì˜ ì°¨ì´ì ì€ NSP (Next Sentence Predict)ë¥¼ Pretrainingì— ì ìš©í•˜ì§€ ì•Šì€ ê²ƒì¸ë°, ìœ ì˜ë¯¸í•œ ì„±ëŠ¥ í–¥ìƒì´ ì—†ì—ˆê¸° ë•Œë¬¸ì´ë¼ê³  í•œë‹¤.</p>

<h3 id="relative-segment-encodings">Relative Segment Encodings</h3>

<p>BERTì˜ segment embeddingì€ \(S_A\)ì™€ \(S_B\) ë“±ìœ¼ë¡œ \(A\)ë¬¸ì¥ì¸ì§€, \(B\)ë¬¸ì¥ì¸ì§€ë¥¼ ë“œëŸ¬ëƒˆë‹¤. XLNetì—ì„œëŠ” Transformer-XLì˜ relative positional encodingì˜ ideaë¥¼ segmentì—ë„ ì ìš©í•´ relativeí•œ ê°’ìœ¼ë¡œ í‘œí˜„í–ˆë‹¤. XLNetì˜ Segment Encodingì€ ë‘ position \(i, j\)ê°€ ê°™ì€ segmentë¼ë©´ \(s_+\), ë‹¤ë¥¸ segmentë¼ë©´ \(s_-\)ë¡œ ì •ì˜ëœë‹¤. \(s_+\)ì™€ \(s_-\)ëŠ” ëª¨ë‘ training ê³¼ì •ì—ì„œ í•™ìŠµë˜ëŠ” parametersì´ë‹¤. ì´ëŸ¬í•œ relative segment encodingì€ ì¬ê·€ì ìœ¼ë¡œ segment encodingì„ ì°¾ì•„ë‚´ë©´ì„œ generalizationëœ í‘œí˜„ì´ ê°€ëŠ¥í•˜ë‹¤ëŠ” ì , ë‘ ê°œ ì´ìƒì˜ segment inputì— ëŒ€í•œ ì²˜ë¦¬ ê°€ëŠ¥ì„±ì„ ì—´ì—ˆë‹¤ëŠ” ì ì—ì„œ ì˜ì˜ê°€ ìˆë‹¤.</p>

<h2 id="discussion">Discussion</h2>

<p>êµ¬ì²´ì ì¸ ì˜ˆì‹œë¥¼ ë“¤ì–´ BERTì™€ ë¹„êµí•´ë³´ì. BERTì™€ XLNetì´ â€œNew York is a city.â€ë¼ëŠ” ë¬¸ì¥ì„ pretrainingí•˜ëŠ” ìƒí™©ì´ë‹¤. [New, York]ì˜ ë‘ tokenì„ predictí•˜ëŠ” ê²ƒì´ ëª©í‘œì´ë‹¤. BERTì˜ objectiveëŠ” ë‹¤ìŒì˜ ìˆ˜ì‹ì´ë‹¤.</p>

\[J_{BERT}=log{\ p\left(New\ |\ is\ a\ city\right)} + log{\ p\left(York\ |\ is\ a\ city\right)}\]

<p>XLNetì€ ìˆœì—´ì„ íŠ¹ì •í•´ì•¼ objectiveë¥¼ êµ¬ì²´í™”í•  ìˆ˜ ìˆë‹¤. [is, a, city, New, York]ì˜ ìˆœì—´ì´ë¼ê³  ê°€ì •í•˜ì. ë‹¤ìŒì˜ ìˆ˜ì‹ì´ XLNetì˜ objectiveì´ë‹¤.</p>

\[J_{XLNet}=log{\ p\left(New\ |\ is\ a\ city\right)} + log{\ p\left(York\ |\ \textbf{New}\ is\ a\ city\right)}\]

<p>XLNetì€ AutoRegressive Modelì´ê¸° ë•Œë¬¸ì— input sentenceì— ë³€í˜•ì„ ê°€í•˜ì§€ ì•Šê³ , ë”°ë¼ì„œ predict target word ì‚¬ì´ì˜ dependency ì—­ì‹œ í•™ìŠµí•  ìˆ˜ê°€ ìˆë‹¤. ìœ„ì˜ ì˜ˆì‹œì—ì„œëŠ” â€˜Yorkâ€™ë¥¼ predictí•  ë•Œì— â€˜Newâ€™ tokenì˜ ì •ë³´ë¥¼ í™œìš©í–ˆë‹¤.</p>

<h1 id="experiments">Experiments</h1>

<h2 id="pretraining-and-implementation">Pretraining and Implementation</h2>

<p>Pretrainingì˜ Datasetìœ¼ë¡œ BooksCorpus, Giga5, CLue Web2012-B, Common Crawl datasetì˜ Datasetì„ ì‚¬ìš©í–ˆë‹¤. Google SentencePiece Tokenizerë¥¼ ì‚¬ìš©í–ˆë‹¤. XLNet-LargeëŠ” 512 TPU v.3ë¥¼ ì‚¬ìš©í•´ 2.5ì¼ë™ì•ˆ 500K stepì˜ í•™ìŠµì„ ì§„í–‰í–ˆë‹¤. ì´ ë•Œ Adam Optimizerë¥¼ ì‚¬ìš©í–ˆë‹¤. Datasetì˜ í¬ê¸°ì— ë¹„í•´ í•™ìŠµëŸ‰ì´ ì ì–´ Unerfittingëœ ìƒíƒœì´ì§€ë§Œ, Pretrainingì„ ë” ìˆ˜í–‰í•œë‹¤ê³  í•˜ë”ë¼ë„ ì‹¤ì œ downstream taskì—ì„œ ìœ ì˜ë¯¸í•œ ì„±ëŠ¥ í–¥ìƒì€ ì—†ì—ˆë‹¤.</p>

<h2 id="fair-comparison-with-bert">Fair Comparison with BERT</h2>

<p><img src="/assets/images/2021-01-19-XLNet-Generalized-Autoregressive-Pretraining-for-Language-Understanding/08-29-2020-22.01.26.jpg" alt="XLNet%20Generalized%20Autoregressive%20Pretraining%20for%20L%20833f510b35954da883906c9bc6b15f9d/08-29-2020-22.01.26.jpg" /></p>

<p>XLNet-LargeëŠ” ëª¨ë“  taskì—ì„œ BERT-Largeë³´ë‹¤ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì˜€ë‹¤.</p>

<h2 id="comparison-with-roberta-scailing-up">Comparison with RoBERTa: Scailing Up</h2>

<p><img src="/assets/images/2021-01-19-XLNet-Generalized-Autoregressive-Pretraining-for-Language-Understanding/08-29-2020-22.01.38.jpg" alt="XLNet%20Generalized%20Autoregressive%20Pretraining%20for%20L%20833f510b35954da883906c9bc6b15f9d/08-29-2020-22.01.38.jpg" /></p>

<p>XLNetì€ RACE taskì—ì„œë„ BERT, GPT, RoBERTa ë“±ì˜ modelë“¤ë³´ë‹¤ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì˜€ë‹¤.</p>

<p><img src="/assets/images/2021-01-19-XLNet-Generalized-Autoregressive-Pretraining-for-Language-Understanding/08-29-2020-22.05.19.jpg" alt="XLNet%20Generalized%20Autoregressive%20Pretraining%20for%20L%20833f510b35954da883906c9bc6b15f9d/08-29-2020-22.05.19.jpg" /></p>

<p>XLNetì€ SQuAD2.0 taskì—ì„œë„ BERT, RoBERTaë³´ë‹¤ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì˜€ë‹¤.</p>

<p><img src="/assets/images/2021-01-19-XLNet-Generalized-Autoregressive-Pretraining-for-Language-Understanding/08-29-2020-22.06.47.jpg" alt="XLNet%20Generalized%20Autoregressive%20Pretraining%20for%20L%20833f510b35954da883906c9bc6b15f9d/08-29-2020-22.06.47.jpg" /></p>

<p>XLNetì€ GLUE taskì—ì„œë„ BERT, RoBERTaë³´ë‹¤ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì˜€ë‹¤.</p>

<h2 id="ablation-study">Ablation Study</h2>

<p><img src="/assets/images/2021-01-19-XLNet-Generalized-Autoregressive-Pretraining-for-Language-Understanding/08-29-2020-22.12.45.jpg" alt="XLNet%20Generalized%20Autoregressive%20Pretraining%20for%20L%20833f510b35954da883906c9bc6b15f9d/08-29-2020-22.12.45.jpg" /></p>

<p>1~4ë¥¼ ì‚´í´ë³´ë©´ XLNet-Baseê°€ BERTë‚˜ Transformer-XLë³´ë‹¤ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì¸ë‹¤. ì´ë¥¼ í†µí•´ permutation language modeling objectiveê°€ íš¨ê³¼ì ì´ì—ˆë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.</p>

<p>1~2ë¥¼ ì‚´í´ë³´ë©´ Transformer-XLì´ BERTë³´ë‹¤ RACEì™€ SQuAD2.0 taskì—ì„œ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì¸ë‹¤. ì´ë¥¼ í†µí•´ Transformer-XL ê³„ì—´ì˜ modelì´ long sequence modelingì— íš¨ê³¼ì ì´ë¼ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.</p>

<p>4~5í–‰ì„ ì‚´í´ë³´ë©´ memory caching mechanismì´ ë¹ ì§„ ê²½ìš° RACEë‚˜ SQuAD2.0ê³¼ ê°™ì€ long sequence taskì—ì„œ ì„±ëŠ¥ ì €í•˜ê°€ ìˆì—ˆë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.</p>

<p>4, 6~7í–‰ì„ ì‚´í´ë³´ë©´ span-based predictionê³¼ bidirectional dataê°€ ì„±ëŠ¥ í–¥ìƒì— ê¸°ì—¬í–ˆë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.</p>

<p>ë§ˆì§€ë§‰ìœ¼ë¡œ 4, 8í–‰ì„ í†µí•´ NSPê°€ RACE taskë¥¼ ì œì™¸í•œ ëª¨ë“  ê²½ìš°ì—ì„œ ì˜¤íˆë ¤ ì„±ëŠ¥ì„ í•˜ë½ì‹œì¼°ë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.</p>

<h1 id="conclusions">Conclusions</h1>

<p>Permutationì„ ì‚¬ìš©í•œ Autoregressive Pretraining ë°©ì‹ì„ ê°œì²™í–ˆë‹¤ëŠ” ì ì—ì„œ ì˜ì˜ê°€ ìˆë‹¤.</p>
:ET